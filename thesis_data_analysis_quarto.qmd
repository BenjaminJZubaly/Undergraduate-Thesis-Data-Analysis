---
title: "Thesis Data Analysis: The Psychology of Scientific Fraud"
author: "Benjamin Zubaly"
date: March 4, 2024
format:
  html:
    self-contained: true
    toc: true
  docx:
    toc: true
  pdf:
    toc: true
editor: visual
csl: apa.csl
bibliography: references.yaml
---

# Introduction

This document is intended to track the data analysis for my undergraduate thesis project on the psychology of scientific fraud. I have already planned out (and preregistered) my data analysis plan (see [here](https://benjaminjzubaly.github.io/The-Psychology-of-Scientific-Fraud-Thesis/#data-analysis "Thesis Data Analysis Plan")), and I will note throughout the document if I deviate from this plan and why. This project is also being tracked in a private GitHub repository in case I need to revert back to a previous version of the project due to a fatal error.

## Overview of Sections

-   **Data Cleaning:** The data cleaning section will take the finalized dataset that I have included in the project directory and use it to create the remaining variables that we need for our analysis. We will also save this dataset.

-   **Data Exploration:** In the data exploration section, we will check for and deal with missing data, run descriptive statistics of our outcome variables, visualize our data, and run bivariate correlations.

-   **Testing Hypotheses:** In the hypothesis testing section, we will test each of our hypotheses one-by-one, according to our analysis plan.

## Directory Set-Up

In order to ensure that this analysis is computationally reproducible, I have included everything that is needed to complete this analysis (just the finalized data file, "study_dataset.csv") in the current working directory. The code will be displayed before the output of each analysis.

## Variable Definitions

Although the following variables may not exist until after the data cleaning section, here is what each variable name refers to, so that you can refer to them while examining the code.

`DOI`: Unique identifier for each paper (i.e., the paper DOI).

`PaperType`: Categorical variable indicating SAFP, SAGP, MAFP, or MAGP.

`LingObf`: Continuous variable for linguistic obfuscation.

`CertSent`: Continuous variable for certainty sentiment.

`Refs`: Count variable for references.

`FraudCorrAuth`: Dichotomous variable indicating if the fraudulent author is the corresponding author (1) or is not (0). Unknown cases will be marked in a seperate variable with this variable left blank.

`NumAuth`: Count variable indicating the number of authors for each paper.

`abstraction`: Abstraction index composed of the sum of standardized scores for `article`, `prep`, and `quantity`.

`article`: Articles from LIWC.

`prep`: Prepositions from LIWC.

`quantity`: Quantities from LIWC.

`cause`: Causation terms from LIWC.

`jargon`: The percent of words not captured by LIWC (100-`Dic`).

`Dic`: The percentage of words captured by all LIWC dictionaries.

`emo_pos`: Positive emotion terms from LIWC.

`flesch_re`: Flesch Reading Ease from ARTE.

## Initial Package Installations

If you would like to reproduce this analysis, here are the package I will be using, so that they can be cued before starting.

```{r}
install.packages("readr")       # For reading data
install.packages("dplyr")       # For data manipulation and handling of missing data
install.packages("psych")       # For descriptive statistics, correlations
install.packages("ggplot2")     # For data visualization
install.packages("car")         # For diagnostic tests such as Levene's test
install.packages("rcompanion")  # For Games-Howell post-hoc test (if applicable)
install.packages("dunn.test")   # For Dunn post-hoc test (if applicable)
```

## Initial Import of Data

We will not load in the dataset "study_dataset.csv" from the working directory.

```{r}
library(readr) # Loading the readr package

data <- read_csv("study_dataset.csv") # Loading in study dataset as "data"
```

I have viewed the data frame, and the data seems to have loaded correctly.

# Data Cleaning

To conduct the analysis, we will first need to calculate the `LingObf` variable by calculating the `abstraction` index and `jargon` words; creating standardized scores for `abstraction`, `cause`, `jargon`, `emo_pos`, and `flesch_re`; and calculating the `LingObf` composite variable from these standardized scores.

1.  First, we will calculate the `abstraction` index by creating standardized scores for `article`, `prep`, and `quantity` and summing them.

```{r}
# Calculate standardized scores for article, prep, and quantity and add them to the dataset
data$articles_standardized <- scale(data$article)
data$prep_standardized <- scale(data$prep)
data$quantity_standardized <- scale(data$quantity)

# Create the new variable 'abstraction' as the sum of the three standardized variables
data$abstraction <- (data$articles_standardized + data$prep_standardized + data$quantity_standardized)
```

-   After viewing the data, the transformations and variable calculation seem to have occurred appropriately.

2.  Next, we will calculate the `jargon` words by subtracting `Dic` from 100.

```{r}
# Calculate the new variable 'jargon' by subtracting 'Dic' from 100
data$jargon <- (100 - data$Dic)
```

-   After viewing the data, the variable calculation seem to have occurred appropriately.

3.  Next, we will create standardized scores for each subcomponent of the `LingObf`.

```{r}
# Standardize the new set of variables and add them to the dataset
data$abstraction_standardized <- scale(data$abstraction)
data$cause_standardized <- scale(data$cause)
data$jargon_standardized <- scale(data$jargon)
data$emo_pos_standardized <- scale(data$emo_pos)
data$flesch_re_standardized <- scale(data$flesch_re)
```

-   After viewing the data, the variable transformations seem to have occurred appropriately.

4.  Now we will calculate the `LingObf` variable using the following formula: \[cause_standardized + abstraction_standardized + jargon_standardized\] – \[emo_pos_standardized + flesch_re_standardized\].

```{r}
# Calculate 'LingObf'
data$LingObf <- (data$cause_standardized + data$abstraction_standardized + data$jargon_standardized) - (data$emo_pos_standardized + data$flesch_re_standardized)
```

-   After viewing the data, the variable calculation seem to have occurred appropriately.

5.  Lastly, our variable that indicates certainty sentiment is currently `certainty_avg`, but to make things easier I am going to copy this data into a new variables called `CertSent`.

```{r}
data$CertSent <- data$certainty_avg
```

To ensure that our clean data is saved, we will write the dataset to the current working directory.

```{r}
# Writing our data as a csv file in the current working directory
write.csv(data, "clean_study_data.csv")
```

-   I have opened the saved data file outside of Rstudio, and it seems to have been written correctly.

# Data Exploration

## Dealing with Missing Data

1.  Data will be first inspected for missing scores.

```{r}
library(dplyr) # Loading the dplyr package for data manipulation and handling missing values

# To summarize the number of missing values in each column
missing_data_summary <- sapply(data, function(x) sum(is.na(x)))

print(missing_data_summary) # To see summary of missing values for all columns
```

-   Taking a look at our outcome variables, there are no missing scores, so we do not need to impute any values.

## Descriptive Statistics
