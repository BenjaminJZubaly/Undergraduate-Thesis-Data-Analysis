---
title: "Thesis Data Analysis: The Psychology of Scientific Fraud"
author: "Benjamin Zubaly"
date: March 4, 2024
format:
  html:
    self-contained: true
    toc: true
  docx:
    toc: true
  pdf:
    toc: true
editor: visual
csl: apa.csl
bibliography: references.yaml
---

```{r setup, include=FALSE}
# Set CRAN mirror
options(repos = list(CRAN = "https://cloud.r-project.org/"))
```

# Introduction

This document is intended to track the data analysis for my undergraduate thesis project on the psychology of scientific fraud. I have already planned out (and [preregistered](https://osf.io/vmq48/?view_only=5d55e73ffc7948748e5a7049c6e52161 "OSF Preregistration Link")) my data analysis plan (see [here](https://benjaminjzubaly.github.io/The-Psychology-of-Scientific-Fraud-Thesis/#data-analysis "Thesis Data Analysis Plan")), and I will note throughout the document if I deviate from this plan and why. This project is also being tracked in a private GitHub repository in case I need to revert back to a previous version of the project due to a fatal error.

## Overview of Sections

-   **Data Cleaning:** The data cleaning section will take the finalized dataset that I have included in the project directory and use it to create the remaining variables that we need for our analysis. We will also save this dataset.

-   **Data Exploration:** In the data exploration section, we will check for and deal with missing data, run descriptive statistics of our outcome variables, visualize our data, and run bivariate correlations.

-   **Testing Hypotheses:** In the hypothesis testing section, we will test each of our hypotheses one-by-one, according to our analysis plan.

## Directory Set-Up

In order to ensure that this analysis is computationally reproducible, I have included everything that is needed to complete this analysis (just the finalized data file, "study_dataset.csv") in the current working directory. The code will be displayed before the output of each analysis.

## Variable Definitions

Although the following variables may not exist until after the data cleaning section, here is what each variable name refers to, so that you can refer to them while examining the code.

`DOI`: Unique identifier for each paper (i.e., the paper DOI).

`PaperType`: Categorical variable indicating SAFP, SAGP, MAFP, or MAGP.

`LingObf`: Continuous variable for linguistic obfuscation.

`CertSent`: Continuous variable for certainty sentiment.

`Refs`: Count variable for references.

`FraudCorrAuth`: Dichotomous variable indicating if the fraudulent author is the corresponding author (1) or is not (0). Unknown cases will be marked in a seperate variable with this variable left blank.

`NumAuth`: Count variable indicating the number of authors for each paper.

`abstraction`: Abstraction index composed of the sum of standardized scores for `article`, `prep`, and `quantity`.

`article`: Articles from LIWC.

`prep`: Prepositions from LIWC.

`quantity`: Quantities from LIWC.

`cause`: Causation terms from LIWC.

`jargon`: The percent of words not captured by LIWC (100-`Dic`).

`Dic`: The percentage of words captured by all LIWC dictionaries.

`emo_pos`: Positive emotion terms from LIWC.

`flesch_re`: Flesch Reading Ease from ARTE.

## Initial Package Installations

If you would like to reproduce this analysis, here are the package I will be using, so that they can be cued before starting.

```{r}
install.packages("readr")       # For reading data
install.packages("dplyr")       # For data manipulation and handling of missing data
install.packages("psych")       # For descriptive statistics, correlations
install.packages("ggplot2")     # For data visualization
install.packages("car")         # For diagnostic tests such as Levene's test
install.packages("effsize")     # For calculating effect sizes
install.packages("rcompanion")  # For Games-Howell post-hoc test (if applicable)
install.packages("dunn.test")   # For Dunn post-hoc test (if applicable)
install.packages("boot")        # For bootstrapped hypothesis tests (if applicable)
```

## Initial Import of Data

We will not load in the dataset "study_dataset.csv" from the working directory.

```{r}
library(readr) # Loading the readr package

data <- read_csv("study_dataset.csv") # Loading in study dataset as "data"
```

I have viewed the data frame, and the data seems to have loaded correctly.

# Data Cleaning

To conduct the analysis, we will first need to calculate the `LingObf` variable by calculating the `abstraction` index and `jargon` words; creating standardized scores for `abstraction`, `cause`, `jargon`, `emo_pos`, and `flesch_re`; and calculating the `LingObf` composite variable from these standardized scores.

1.  First, we will calculate the `abstraction` index by creating standardized scores for `article`, `prep`, and `quantity` and summing them.

```{r}
# Calculate standardized scores for article, prep, and quantity and add them to the dataset
data$articles_standardized <- scale(data$article)
data$prep_standardized <- scale(data$prep)
data$quantity_standardized <- scale(data$quantity)

# Create the new variable 'abstraction' as the inverse of the sum of the three standardized variables
data$abstraction <- (1/(data$articles_standardized + data$prep_standardized + data$quantity_standardized))
```

-   After viewing the data, the transformations and variable calculation seem to have occurred appropriately.

2.  Next, we will calculate the `jargon` words by subtracting `Dic` from 100.

```{r}
# Calculate the new variable 'jargon' by subtracting 'Dic' from 100
data$jargon <- (100 - data$Dic)
```

-   After viewing the data, the variable calculation seem to have occurred appropriately.

3.  Next, we will create standardized scores for each subcomponent of the `LingObf`.

```{r}
# Standardize the new set of variables and add them to the dataset
data$abstraction_standardized <- scale(data$abstraction)
data$cause_standardized <- scale(data$cause)
data$jargon_standardized <- scale(data$jargon)
data$emo_pos_standardized <- scale(data$emo_pos)
data$flesch_re_standardized <- scale(data$flesch_re)
```

-   After viewing the data, the variable transformations seem to have occurred appropriately.

4.  Now we will calculate the `LingObf` variable using the following formula: \[cause_standardized + abstraction_standardized + jargon_standardized\] – \[emo_pos_standardized + flesch_re_standardized\].

```{r}
# Calculate 'LingObf'
data$LingObf <- (data$cause_standardized + data$abstraction_standardized + data$jargon_standardized) - (data$emo_pos_standardized + data$flesch_re_standardized)
```

-   After viewing the data, the variable calculation seem to have occurred appropriately.

5.  Lastly, our variable that indicates certainty sentiment is currently `certainty_avg`, but to make things easier I am going to copy this data into a new variables called `CertSent`.

```{r}
data$CertSent <- data$certainty_avg
```

To ensure that our clean data is saved, we will write the dataset to the current working directory.

```{r}
# Writing our data as a csv file in the current working directory
write.csv(data, "clean_study_data.csv")
```

-   I have opened the saved data file outside of Rstudio, and it seems to have been written correctly.

# Data Exploration

## Missing Data

1.  Data will be first inspected for missing scores.

```{r}
library(dplyr) # Loading the dplyr package for data manipulation and handling missing values

# To summarize the number of missing values in each column
missing_data_summary <- sapply(data, function(x) sum(is.na(x)))

print(missing_data_summary) # To see summary of missing values for all columns
```

-   Taking a look at our outcome variables, there are no missing scores, so we do not need to impute any values.

## Descriptive Statistics

1.  I will now generate descriptive statistics for each relevant variable in the dataset. I originally (in the preregistered plan) was simply going to deploy the `describe()` function on the entire dataset, but because I retained all of the columns from the Retraction Watch Database [@retracti2023] and all of the output from the text analysis packages [@aggarwal2022; @boyd2022; @rocklage2023] there are currently 219 variables. Because this would be unmanageable, I am going to only calculate descriptive statistics for a selection of variables of interest. I will first create a dataframe with only the continuous variables that I am interested in generating descriptive statistics for, and I will use the `psych` package to produce the descriptive statistics for these variables.

```{r}
library(psych) # Loading the psych package

# Selecting the continuous variables I am interested in getting descriptive statistics for (plus paper type for the next part)
continuous_data_for_descriptives <- data[c("year", "Refs", "flesch_re", "WC", "abstraction", "jargon", "CertSent", "LingObf", "cause", "emo_pos", "article", "prep", "quantity", "PaperType")]

# Generating descriptive statistics for variables of interest
continuous_descriptive_stats_all <- describe(continuous_data_for_descriptives)

# Displaying the results of the descriptive stats for the variables listed above
continuous_descriptive_stats_all
```

-   I won't make too many comments here, because for most of these measures there are no formal or informal norms against which to judge them. That being said:
    -   **Year:** The mean year is mid-2009, with a standard deviation of 10 years (max 2022 and min 1980), indicating that the sample is recent enough to be relevant but also spans quite a number of years. This I think is good insofar as the recent history of academic publishing is represented more fully (some recent investigations limited their search to the three years prior to publication). There is some negative skew, which I suspect is due to the 1980 paper being quite a bit older than most papers.
    -   **Refs:** The mean number of references is 45.82 with a standard deviation of 23.47. At least intuitively, this seems like a pretty standard distribution of references if we were to randomly select papers from the literature. However, the range is huge, with one paper showing 146 references—perhaps why the skew is positive. This may be something to consider when making group comparisons, as this point may have significant leverage.
    -   **WC:** The average word count is 4436.25, and the standard deviation is 2248.24. It is also positively skewed, and although this should not affect our linguistic dependent variables it may be a confounding factor for references. Indeed, taking a look at the dataset I can see that the second highest value for word count (9670) also has the maximum value for references (the outlier noted above at 146 references). This will be something to watch out for.
-   Now we will create descriptive statistics for each of the continuous variables above within the `PaperType` groups.

```{r}
# Making PaperType a factor variable in the continuous variable dataframe to allow for grouping
continuous_data_for_descriptives$PaperType <- factor(continuous_data_for_descriptives$PaperType, levels = c("SAFP", "MAFP", "SAGP", "MAGP"), labels = c("Single-Authored Fraudulent Papers", "Multi-Authored Fraudulent Papers", "Single-Authored Genuine Papers", "Multi-Authored Genuine Papers"))

# Generating descriptive statistics within PaperType groups
descriptive_stats_by_PaperType <- describeBy(continuous_data_for_descriptives, group = continuous_data_for_descriptives$PaperType)

descriptive_stats_by_PaperType
```

-   Some quick notes:

    -   **Year:** The year seems to be relatively similar between groups, with the single-author groups (SAGP and SAFP) both having mid-2008 as the mean and the multi-author groups both having mid-2010 as the mean. This may be partially due to there being single outliers (matched papers that are both outliers) at the lower end of the distribution.

    -   **Refs:** As I suspected, the outlier of 146 seems to be pulling the mean for SAGP up to 51.05 (the means for all other groups are around 44). This is corroborated by the skew statistics which show that only the SAGP group has a skew value above 1. I may run the analyses where references is the outcome variable both with and without this case in the model, or I may use word count as a covariate in the model to try to account for it.

    -   **WC:** For some reason, there seems to be a notable difference in mean word count between the single-authored fraudulent group and the rest of the groups. I could speculate as to why this is the case, but I may do an unplanned follow-up analysis of this.

2.  Frequencies will now be produced for categorical variables, both for the data in general and within `PaperType` groups.

-   First, we will make the variables `inst_pres`, `gender`, `simple_reason`, `Country`, and `PaperType` factor variables.

```{r}
# Changing inst_pres (institutional prestige), gender, simple_reason, Country, and PaperType into factor variables
data$inst_pres <- factor(data$inst_pres, levels = c(0, 1), labels = c("Not Major Research Institution", "Major Research Institution"))
data$gender <- factor(data$gender, levels = c("FEMALE", "MALE"), labels = c("Female", "Male"))
data$simple_reason <- factor(data$simple_reason, levels = c("f_data", "f_image", "m_image", "f_data f_image"), labels = c("Fabricated/Falsified Data", "Fabricated/Falsified Image", "Manipulated Image", "Fabricated/Falsified Data and Image"))
data$Country <- factor(data$Country)
data$PaperType <- factor(data$PaperType)
```

-   Next, we will produce frequency tables for each categorical variable of interest for the whole dataset.

```{r}
# Creating the frequency tables for each categorical variable
freq_tab_inst_pres <- table(data$inst_pres)
freq_tab_gender <- table(data$gender)
freq_tab_simple_reason <- table(data$simple_reason)
freq_tab_Country <- table(data$Country)
freq_tab_PaperType <- table(data$PaperType)

# Displaying the frequency tables
freq_tab_inst_pres
freq_tab_gender
freq_tab_simple_reason
freq_tab_Country
freq_tab_PaperType
```

-   Let's take a look at the way these are split:

    -   First, there seems to ba a preponderance of lower status (not major) research institutions in the whole dataset (70:18).

    -   Next, most of the papers were retracted due to fabricated/falsified data (*n* = 27), followed by those that manipulated images (*n* = 12), fabricated/falsified images (*n* = 3), or both fabricated/falsified data and images (*n* = 2).

    -   For country, the most common category of papers seem to be from the United States (*n* = 29), followed by China and India (both *n* = 10) and the Netherlands (*n* = 6), with no other countries totaling more than *n* = 4.

    -   Finally, we can see that the papers are evenly split between each of our groups (*n* = 22; *N* = 88).

-   Next, we will produce the frequencies within `PaperType` groups. Because we were not able to perfectly match across our matching characteristics, we will take this opportunity to look at how far off we were. A proportion table will be produced to more easily compare frequencies across `PaperType` groups. First, however, we need to make the matching dummy variables into factor variables with labels to make them more interpretable.

```{r}
# Creating factor variables out of the dummy variables that track matching across groups
data$different_country <- factor(data$different_country, levels = c(0, 1), labels = c("Same Country", "Different Country"))
data$different_gender <- factor(data$different_gender, levels = c(0, 1), labels = c("Same Gender", "Different Gender"))
data$different_inst_pres <- factor(data$different_inst_pres, levels = c(0, 1), labels = c("Same Institutional Prestige", "Different Institutional Prestige"))
data$different_journal <- factor(data$different_journal, levels = c(0, 1), labels = c("Same Journal", "Different Journal"))
```

-   I have checked the data set, and the labels seem to have been assigned correctly. Now we will produce the frequency tables and proportion tables.
    -   Because we are taking a look at matching characteristics here, I will also take a look at the `year_difference` (the difference between the years of publication for the matched papers) variable within each group by using the `describe()` function from the `psych` package (like for the continuous variables above). In addition, I will take a look at the `NumAuth` (number of authors) for each of the groups.

```{r}
# Frequency tables and proportion tables
  # institutional prestige
frequency_table_by_PaperType_inst_pres <- table(data$PaperType, data$inst_pres)
frequency_table_by_PaperType_inst_pres
proportion_table_by_PaperType_inst_pres <- prop.table(frequency_table_by_PaperType_inst_pres, margin = 1)
proportion_table_by_PaperType_inst_pres
  # gender
frequency_table_by_PaperType_gender <- table(data$PaperType, data$gender)
frequency_table_by_PaperType_gender
proportion_table_by_PaperType_gender <- prop.table(frequency_table_by_PaperType_gender, margin = 1)
proportion_table_by_PaperType_gender
  # reason
frequency_table_by_PaperType_reason <- table(data$PaperType, data$simple_reason)
frequency_table_by_PaperType_reason
proportion_table_by_PaperType_reason <- prop.table(frequency_table_by_PaperType_reason, margin = 1)
proportion_table_by_PaperType_reason
  # country
frequency_table_by_PaperType_country <- table(data$PaperType, data$Country)
frequency_table_by_PaperType_country
proportion_table_by_PaperType_country <- prop.table(frequency_table_by_PaperType_country, margin = 1)
proportion_table_by_PaperType_country

# And for the matching characteristic dummy variables
  # Country different
frequency_table_by_PaperType_different_country <- table(data$PaperType, data$different_country)
frequency_table_by_PaperType_different_country
proportion_table_by_PaperType_different_country <- prop.table(frequency_table_by_PaperType_different_country, margin = 1)
proportion_table_by_PaperType_different_country
  # Gender different
frequency_table_by_PaperType_different_gender <- table(data$PaperType, data$different_gender)
frequency_table_by_PaperType_different_gender
proportion_table_by_PaperType_different_gender <- prop.table(frequency_table_by_PaperType_different_gender, margin = 1)
proportion_table_by_PaperType_different_gender
  # Institutional prestige different
frequency_table_by_PaperType_different_inst_pres <- table(data$PaperType, data$different_inst_pres)
frequency_table_by_PaperType_different_inst_pres
proportion_table_by_PaperType_different_inst_pres <- prop.table(frequency_table_by_PaperType_different_inst_pres, margin = 1)
proportion_table_by_PaperType_different_inst_pres
  # Journal different
frequency_table_by_PaperType_different_journal <- table(data$PaperType, data$different_journal)
frequency_table_by_PaperType_different_journal
proportion_table_by_PaperType_different_journal <- prop.table(frequency_table_by_PaperType_different_journal, margin = 1)
proportion_table_by_PaperType_different_journal

# Descriptive statistics for year off within PaperType groups
year_difference_descriptives <- describeBy(data$year_difference, group = data$PaperType, na.rm = TRUE)
year_difference_descriptives
num_auth_descriptives <- describeBy(data$NumAuth, group = data$PaperType, na.rm = TRUE)
num_auth_descriptives
```

-   **Overview of Categorical Variables:**

    -   **Institutional Prestige:** The split for frequencies/proportion of is very close to even across groups. The MAFP and SAGP groups have 17 papers from major research institutions and 5 from lower status research institutions (.77/.22), and the MAGP and SAFP groups have 18 papers from major research institutions and 4 from lower status research institutions (.82/.18).

    -   **Gender:** The split between groups is also very similar for gender, with MAFP, MAGP, and SAGP groups each having 4 female first authors and 18 male first authors (.18/.82). SAFP had 3 female first authors and 19 male first authors (.14/.86).

    -   **Reason (Not Matched Between Groups During Data Collection):** The reason for retraction is only relevant for the fraudulent paper groups, and there is a similar split between them. The SAFP had 15 papers in the group for Fabricated/Falsified Data, 2 paper for Fabricated/Falsified Images, 5 for Manipulated Images, and 0 that Fabricated/Falsified Data and Images (.68/.09/.22/.00). The MAFP had 12 papers in the group for Fabricated/Falsified Data, 1 paper for Fabricated/Falsified Images, 7 for Manipulated Images, and 2 for Fabricated/Falsified Data and Images (.55/.05/.32/.09).

    -   **Country:** Because there are many countries, I will not delineate them all here but rather refer you to the data printed above if you are interested in diving deeper. To highlight one thing, we can see that papers from the United States (our most common country in the whole data set) tend to be overrepresented in the frequencies for each group (MAFP, 8/.36; MAGP, 9/.40; SAFP, 7/.32; SAGP, 5/.23). There are no other countries with proportions that exceed the lowest proportion of United States papers in any group (all\<.23).

-   **Overview of Matching Characteristic Dummy Variables and Average Differences:**

    -   Although the frequencies and proportions of the matched variables within groups above indicates to some degree how similar each group is, the more important metric of how closely matched the groups are is between the groups for which comparisons are to be made. For this reason, I dummy coded whether each paper matched it's pair on each characteristic or not as a binary variable (which we labeled above) before loading the data. Because the SAFP papers were not gathered by matching to any group, there are no data for the SAFP group for any of the dummy variables, and journal only didn't match across some of the SAFP and MAFP pairs. Each of the other groups is in reference to the group against which it will be compared (i.e., data for MAFP represent how well they match SAFP papers, data for MAGP represent how well they match with MAFP, data for SAGP represent how well they match with SAFP). In addition, I coded how different the year was between papers, such that positive values indicate the number of years the original paper was published after the matched paper (negative values indicate the matched paper was published after the paper it was being matched to).

    -   **MAFP compared with SAFP:**

        -   Country was the same for 14 (.64) pairs and different for 8 (.36) pairs.

        -   Gender was the same for 18 (.82) pairs and different for 4 (.18) pairs.

        -   Institutional prestige was the same for 17 (.77) pairs and different for 5 (.23) pairs.

        -   Journal was the same for 11 (.50) pairs and different for 11 (.50) pairs.

        -   The average year difference was -1.36, indicating that the MAFP tended to be slightly more recent than the SAFP.

        -   Finally, the mean number of authors for MAFP is 3.73.

    -   **MAGP compared with MAFP:**

        -   Country was the same for 21 (.95) pairs and different for 1 (.05) pairs.

        -   Gender was the same for 20 (.91) pairs and different for 2 (.09) pairs.

        -   Institutional prestige was the same for 21 pairs (.95) and different for 1 (.05) pair.

        -   Journal was the same for every paper.

        -   The average year difference was -.64, indicating that the MAGP tended to be slightly more recent than the MAFP.

        -   Finally, the mean number of authors for MAFP is 4.05.

    -   **SAGP compared with SAFP:**

        -   Country was the same for 10 (.45) pairs and different for 12 (.55) pairs.

        -   Gender was the same for 19 (.86) pairs and different for 3 (.14) pairs.

        -   Institutional prestige was the same for 21 pairs (.95) and different for 1 (.05) pair.

        -   Journal was the same for every paper.

        -   The average year difference was 0, indicating that the SAGP and SAFP are exactly matched for the average year of publication.

-   **Summary of matching characteristics:**

    -   Looking at the data, the most difficult characteristic to match was country across all categories of papers, except for MAFP compared with SAFP, where journal was the most difficult to match. Nevertheless, for many of the situations where we were unable to match the specific paper, this was balanced out by not being able to match another paper in the opposite direction, as the frequencies within paper types demonstrates.

## Data Visualization

1.  Using the `ggplot2` package, frequency distributions and box plots will be generated for `LingObf`, `CertSent`, and `Refs`. A bar plot will be produced for `FraudCorrAuth`.

```{r}
library(ggplot2) # To load the ggplot2 package

# To produce histogram and box plot for LingObf
ggplot(data, aes(x = LingObf)) + 
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of Linguistic Obfuscation for Entire Dataset")
ggplot(data, aes(y = LingObf)) + 
  geom_boxplot(fill = "lightblue", color = "black") +
  labs(title = "Box Plot of Linguistic Obfuscation for Entire Dataset")

# To produce histogram and box plot for CertSent
ggplot(data, aes(x = CertSent)) + 
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of Certainty Sentiment for Entire Dataset")
ggplot(data, aes(y = CertSent)) + 
  geom_boxplot(fill = "lightblue", color = "black") +
  labs(title = "Box Plot of Certainty Sentiment for Entire Dataset")

# To produce histogram and box plot for Refs
ggplot(data, aes(x = Refs)) + 
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of References for Entire Dataset")
ggplot(data, aes(y = Refs)) + 
  geom_boxplot(fill = "lightblue", color = "black") +
  labs(title = "Box Plot of References for Entire Dataset")

# To produce bar plot for FraudCorrAuth
ggplot(data, aes(x = FraudCorrAuth)) + 
  geom_bar(fill = "coral") +
  labs(title = "Bar Plot of Fraudulent Corresponding Author for MAFP")
```

-   **LingObf:** Linguistic obfuscation seems to have a relatively normal distribution with a somewhat strange jump in frequencies around 2.5 and a somewhat strange drop off in frequencies right above zero (M = 0, SD = 2.04; from descriptives above). The box plot shows a couple of potential outliers, but there are both high (2) and low (2) outliers, which may almost cancel eachother out in terms of being problematic.

-   **CertSent:** Certainty sentiment does not seem to be skewed despite having an outlier at the low end of the distribution as well. The box plot highlights three values at the low end of the scale as being outliers.

-   **Refs:** References has a strange distribution, one that looks like it is actually composed of two latent classes. It also has two outlier values way at the high end of the distribution, as shown by the histogram and the box plot.

-   **FraudCorrAuth:** The fraudulent corresponding author variable shows an even split between papers for which the fraudulent author was and was not the corresponding author.

2.  Using the `ggplot2` package, frequency distributions and box plots will be generated for `LingObf`, `CertSent`, and `Refs` within `PaperType` groups.

```{r}
# To produce histogram and box plot for LingObf
ggplot(data, aes(x = LingObf)) + 
  geom_histogram(fill = "blue", color = "black") +
  facet_wrap(~ PaperType) +
  labs(title = "Histograms of Linguistic Obfuscation for Entire Dataset")
ggplot(data, aes(y = LingObf)) + 
  geom_boxplot(fill = "lightblue", color = "black") +
  facet_wrap(~ PaperType) +
  labs(title = "Box Plots of Linguistic Obfuscation Within Paper Type Groups")

# To produce histogram and box plot for CertSent
ggplot(data, aes(x = CertSent)) + 
  geom_histogram(fill = "blue", color = "black") +
  facet_wrap(~ PaperType) +
  labs(title = "Histograms of Certainty Sentiment Within Paper Type Groups")
ggplot(data, aes(y = CertSent)) + 
  geom_boxplot(fill = "lightblue", color = "black") +
  facet_wrap(~ PaperType) +
  labs(title = "Box Plots of Certainty Sentiment Within Paper Type Groups")

# To produce histogram and box plot for Refs
ggplot(data, aes(x = Refs)) + 
  geom_histogram(fill = "blue", color = "black") +
  facet_wrap(~ PaperType) +
  labs(title = "Histograms of References Within Paper Type Groups")
ggplot(data, aes(y = Refs)) + 
  geom_boxplot(fill = "lightblue", color = "black") +
  facet_wrap(~ PaperType) +
  labs(title = "Box Plots of References Within Paper Type Groups")
```

-   A few quick notes:

    -   **LingObf:**

        -   Outliers for linguistic obfuscation seem to be represented in each of the paper groups except for MAFP.

    -   **CertSent:**

        -   The otulier at the low end of the certainty sentiment distribution seems to belong to the MAGP group.

    -   **Refs:**

        -   The positive outliers we have noted a couple times now for references seem to be in the SAGP group, corroborating our speculation above (see [Descriptive Statistics]).

        -   Strangely, we noted that the references seemed to have two latent classes, but, at least visually, this does not seem to represent any particular paper group, as the latent classes show up in the SAFP and MAGP groups (as well as perhaps the SAGP group). I am not sure why this is.

    -   Overall, the strange values are nto all within one group, which suggests that they are due to random variation rather than one paper that was not read appropriately by the text analysis programs (or some other form of systematic problem with one or two observations in the data set).

## Bivariate Correlations

In order to further explore data characteristics, bivariate correlations between outcome variables will be produced. As these are only for exploring data and not testing predictions, assumptions will not be tested, and the relationships will not be probed for statistical significance. Then, correlations for subcomponents of composite variables will be produced.

1.  First, Pearson bivariate correlations will be produced for `LingObf`, `CertSent`, and `Refs`.

```{r}
# To produce correlation matrix for LingObf, CertSent, and Refs variables within whole dataset
  # To calculate the Pearson correlation coefficients between the numerical outcome variables
correlation_matrix_num_outcomes <- cor(data[c("LingObf", "CertSent", "Refs")], use = "complete.obs", method = "pearson")
  # To display the correlation matrix for numerical outcome variables
correlation_matrix_num_outcomes
```

-   All of the variables here show negligible bivariate relationships, and none of them are significant (I produced the correlation table from another script to see the significance).

2.  Next, here are the point-biserial correlations between `FraudCorrAuth` and `LingObf`, `CertSent`, and `Refs`.

```{r}
# Calculating Point-biserial correlations between FraudCorrAuth and numerical outcome variables
pbcorr_FraudCorrAuth_LingObf <- cor(data$FraudCorrAuth, data$LingObf, use = "complete.obs", method = "pearson")
pbcorr_FraudCorrAuth_CertSent <- cor(data$FraudCorrAuth, data$CertSent, use = "complete.obs", method = "pearson")
pbcorr_FraudCorrAuth_Refs <- cor(data$FraudCorrAuth, data$Refs, use = "complete.obs", method = "pearson")

# Displaying the correlation coefficients
pbcorr_FraudCorrAuth_LingObf
pbcorr_FraudCorrAuth_CertSent
pbcorr_FraudCorrAuth_Refs
```

-   While the relationship between FraudCorrAuth and LingObf is large (r = .57) and the relationship between FraudCorrAuth and CertSent is very large (r = .83), I will refrain from interpreting these because the sample is severely underpowered with a mere *n* = 8 observations.

3.  Now, we will produce bivariate correlations for the subcomponents of the `abstraction` index (see [Data Cleaning] for more details.

```{r}
# Calculate pairwise correlations among the subcomponents of the abstraction index
correlation_matrix_abstraction_subcomponents <- cor(data[,c("article", "prep", "quantity")])

# Display the correlation matrix
correlation_matrix_abstraction_subcomponents
```

-   The correlations here (.03 \< r \< .1) are quite a bit smaller (i.e., nonexistent) than @markowitz2016 found for their sample when they constructed the abstraction index (.176 \< r \< .263). This calls into question the reliability of the abstraction index as a subcomponent of the linguistic obfuscation index in our sample, and it calls into question the validity of each of the indicators of the abstraction index (articles, prepositions, and quantity words) in our sample.

4.  Now we will produce Pearson bivariate correlations for the subcomponents of the `LingObf` index.

```{r}
# Calculate pairwise correlations among the subcomponents of LingObf
correlation_matrix_LingObf_subcomponents <- cor(data[,c("cause", "abstraction", "jargon", "emo_pos", "flesch_re")])

# Display the correlation matrix
correlation_matrix_LingObf_subcomponents
```

-   If we let the Linguistic Obfuscation Index be *O*, causal terms be *C*, the abstraction index be *A*, jargon be *J*, positive emotion terms be *P*, and Flesch reading ease be *F*, the mathematical model for the construction of the Linguistic Obfuscation index for an observation *i* can be stated as follows:

$$
O_i = (C_i + A_i + J_i) - (P_i + F_i)
$$

-   If this model is internally consistent, we should expect that each of the variables within the aggregated variables (i.e., that are within parentheses with one another) should be positively correlate. We should also expect that variables will be negatively correlated across the aggregates (i.e., each of the variables that are in opposite aggregates).

    -   Expected Positive Correlations:

        -   We should expect the relationships between causal terms, the abstraction index, and jargon words to be positive. However, the relationships are (small) negative relationships, with the exception of the relationship between abstraction and jargon, which is very slightly positive.

        -   We also should expect the relationship between positive emotion words and Flesch reading ease to be positive, but it is also a small, negative correlation.

    -   Expected Negative Correlations:

        -   As we should expect, there is a moderate negative relationship between causal terms and Flesch reading ease.

        -   However, we should expect the relationship between causal terms and positive emotion terms to be negative, but it is a small, positive correlation.

        -   We should expect a negative relationship between the abstraction index and positive emotion terms, but there is no relationship to speak of.

        -   We also should expect a negative relationship between the abstraction index and Flesch reading ease, but there is no relationship to speak of (very, very slightly positive).

        -   As should expect, there is a small negative relationship between Jargon and positive emotion terms.

        -   Lastly, we should expect a negative correlation between jargon and Flesch reading ease, but there is a small, positive correlation between them.

    -   The observed relationships here may be expected *a priori* in some cases (e.g., the relationship between jargon and Flesch reading ease may be expected to be a positive relationship because the longer sentences someone writes the more likely they may be to use nonstandard words, simply by volume). However, from an internal consistency perspective, most of the relationships (7/10) we have observed here are either in the wrong direction or are too small to be considered.

        -   This warrants serious consideration for whether our findings for hypothesis A can be interpreted appropriately.

# Testing Hypotheses

### **Hypothesis 1: Linguistic Obfuscation Hypothesis**

**Hypothesis 1** consists of two variants. The first, **Hypothesis 1a**, is the general linguistic obfuscation hypothesis tested by @markowitz2016 which we will test in order to replicate their previous findings. The second, **Hypothesis 1b**, consists of our specific variant of the linguistic obfuscation hypothesis that states fraudulent scientists obfuscate their writing to make their work less accessible to their research group, rather than those outside their research group. This leads to the prediction that SAFP—which presumably are written without the presence of a research group—will use less linguistic obfuscation. Specifically, these hypotheses are stated follows:

**Hypothesis 1a:** Fraudulent research will be written with more linguistic obfuscation than non-fraudulent research. \[Replication\]

**Hypothesis 1b:** Single-author fraudulent research will be written with more linguistic obfuscation than non-fraudulent research but less linguistic obfuscation than multi-author fraudulent research. \[Novel\]

#### **Testing Hypothesis 1a**

To test **Hypothesis 1a**, we will conduct an independent samples t-test comparing the mean levels of `LingObf` of fraudulent papers (SAFP and MAFP combined) with genuine papers (SAGP and MAGP combined). That is, we will see whether the mean linguistic obfuscation differs between fraudulent papers and genuine papers.

1.  To compare fraudulent publications with genuine publications, we will first add a new dichotomous grouping variable for genuine or fraudulent papers (`Genuine_or_Fraudulent`). That is, the new variable will combine SAGP and MAGP into one category (`GPaper`) and SAFP and MAFP into another category (`FPaper`). This will be done using the `dplyr` package. Then we will make the variable a factor variable.

```{r}
data$Genuine_or_Fraudulent <- ifelse(data$PaperType %in% c("SAFP", "MAFP"), "FPaper", "GPaper")
```

-   I have checked the data frame, and this transformation seems to have been done correctly.

-   Now to make it a factor variable.

```{r}
data$Genuine_or_Fraudulent <- factor(data$Genuine_or_Fraudulent, levels = c("FPaper", "GPaper"), labels = c("Fraudulent Papers", "Genuine Papers"))
```

-   This transformation also seems to have been done correctly.

2.  To check the assumption of normality we will use the `ggplot2` package to create Q-Q plots for `LingObf` within the `FPaper`and `GPaper` groups. The Q-Q plots will be investigated visually. Because each group will has *n* = 44 cases, our t-test will not necessarily be rhobust to violations of this assumption. If the assumption of normality is violated and we cannot fix it, then we will use a bootstrapped t-test to fit a more robust model.

```{r}
# To produce Q-Q plots for LingObf within the FPaper and GPaper groups
ggplot(data, aes(sample = LingObf)) + 
  stat_qq() + 
  facet_wrap(~ Genuine_or_Fraudulent) +
  geom_qq_line()
```

-   The QQ plots are not that close to the theoretical values, with deviation at the ends of the distributions (particularly in the fraudulent paper group). Because of this, I will run a Shapiro-Wilk test on each of the groups.

```{r}
# Loading the dplyr package for the pipe function
library(dplyr)

# Performing the Shapiro-Wilk test for normality within each group
sw_results_LingObf_GorP <- data %>%
  group_by(Genuine_or_Fraudulent) %>%
  summarise(SW_test_result = list(shapiro.test(LingObf)))

# Displaying the results
sw_results_LingObf_GorP$SW_test_result[[1]]
sw_results_LingObf_GorP$SW_test_result[[2]]
```

-   The Shapiro-Wilk tests indicate that for both groups we should accept the null hypothesis that the data are normally distributed. Our assumption of normality is therefore supported.

2.  We will now run Levene's test for homogeneity of variance.

```{r}
# To load the car package for Levene's test
library(car)

# Converting LingObf to numeric again, becuase there is a bug that won't allow me to run it if I don't do this
data$LingObf <- as.numeric(data$LingObf)

# To conduct Levene's test for homogeneity of variances
hyp_1a_levene_test <- leveneTest(LingObf ~ Genuine_or_Fraudulent, data = data)

# To display the Levene's test result
hyp_1a_levene_test
```

-   Results of Levene's test indicate that we should accept the null hypothesis that the variances are equal between the groups.

3.  Now we will run the independent samples t-test with bootstrapped confidence intervals.

```{r}
# Load effsize package for Cohen's d
library(effsize)

# Performing the t-test
stud_t_test_obf <- t.test(LingObf ~ Genuine_or_Fraudulent, data = data)

# Calculating Cohen's d using the effsize package
d_obf <- cohen.d(LingObf ~ Genuine_or_Fraudulent, data = data)

# Reporting the results
cat("Mean Difference: ", stud_t_test_obf$estimate, "\n",
    "t-value: ", stud_t_test_obf$statistic, "\n",
    "p-value: ", stud_t_test_obf$p.value, "\n",
    "Cohen's d: ", d_obf$estimate, "\n")
```

-   The results of the t-test do not provide reliable evidence against the null for **Hypothesis 1a**. We should accept the null hypothesis that the means of the two groups are the same.

#### **Testing Hypothesis 1b**

To test **Hypothesis 1b** we will conduct a one-way analysis of variance (ANOVA) to compare group means of `LingObf` for SAFP, MAFP, SAGP, and MAGP. That is, we will determine whether SAFP contains more linguistic obfuscation than the genuine paper groups but less linguistic obfuscation than the MAFP.

1.  To check the assumption of normality we will use the `ggplot2` package to create Q-Q plots for `LingObf` using our original study dataset. The Q-Q plots will be investigated visually. If the assumption of normality is violated we will use a bootstrapped t-test to fit a more robust model.

```{r}
# To produce Q-Q plots for LingObf within the PaperType groups
ggplot(data, aes(sample = LingObf)) + 
  stat_qq() + 
  facet_wrap(~ PaperType) +
  geom_qq_line()
```

-   Although they don't look terrible, I am not satisfied with the plots within PaperType groups for SAFP, MAGP, or SAGP. Therefore, we will test for non-normality using a Shapiro-Wilk test within each group.

```{r}
# Performing the Shapiro-Wilks test for normality within each group
sw_results_LingObf_PT <- data %>%
  group_by(PaperType) %>%
  summarise(SW_test_result2 = list(shapiro.test(LingObf)))

# Displaying the results
sw_results_LingObf_PT$SW_test_result2[[1]]
sw_results_LingObf_PT$SW_test_result2[[2]]
sw_results_LingObf_PT$SW_test_result2[[3]]
sw_results_LingObf_PT$SW_test_result2[[4]]
```

-   The results for the Shapiro-Wilk test for normality indicate that we should accept the null hypothesis that the data are normally distributed for each group. We will now check the assumption of homogeneity of variances.

2.  Running Levene's test for homogeneity of variances for the `PaperType` groups.

```{r}
# To conduct Levene's test for homogeneity of variances
hyp_1b_levene_test <- leveneTest(LingObf ~ PaperType, data = data)

# To display the Levene's test result
hyp_1b_levene_test
```

-   The result of Levene's test indicates that we may accept the null hypothesis that the variances are equal between the groups. Therefore, we will consider the assumption of homogeneity supported and run our one-way ANOVA.

2.  We will now run the one-way ANOVA.

```{r}
# Perform and print the ANOVA
aov_obf <- aov(LingObf ~ PaperType, data=data)
summary(aov_obf)
```

-   The results of this bootstrapped one-way ANOVA indicate that we should accept the null hypothesis that there are no differences between the `PaperType` groups on the `LingObf` variable. No post-hoc test needs to be conducted.

### **Hypothesis 2: References Hypothesis**

**Hypothesis 2** also consists of two variants. The first is that fraudulent research will contain more references than non-fraudulent research, functioning to make the research more costly to assess from outside readers [@markowitz2016] or as an analogue to third-person pronoun usage in other linguistic studies of deception [@schmidt2022]. The second is our adaptation of the first version which emphasizes the salience of the research group as the audience of this communicative style, similar to the logic of Hypothesis 1b. Specifically, the hypotheses are as follows:

**Hypothesis 2a:** Fraudulent research will contain more references than non-fraudulent research. \[Replication\]

**Hypothesis 2b:** Single-author fraudulent research will contain more references than non-fraudulent research but fewer references than multi-author fraudulent research. \[Novel\]

#### **Testing Hypothesis 2a**

To test **Hypothesis 2a**, we will compare the mean number of `Refs` between the `FPaper` and `GPaper` groups using an independent-samples t-test.

1.  To check the assumption of normality we will use the `ggplot2` package to create Q-Q plots for `Refs` within the `Genuine_or_Fraudulent` dichotomous variable. The Q-Q plots will be investigated visually. As references are technically count, rather than continuous, data, there is a higher likelihood that it will violate the assumption of normality. If this is the case we will instead run a Mann-Whitney U test to compare the two groups by their median `Refs`.

```{r}
# To produce Q-Q plots for Refs within the FPaper and GPaper groups
ggplot(data, aes(sample = Refs)) + 
  stat_qq() + 
  facet_wrap(~ Genuine_or_Fraudulent) +
  geom_qq_line()
```

-   The QQ-plots do not look promising, especially for genuine papers, which we noted likely contained outlier values before. We will test normality more formally with a Shapiro-Wilks test.

```{r}
# Performing the Shapiro-Wilks test for normality within each group
sw_results_Refs_GorP <- data %>%
  group_by(Genuine_or_Fraudulent) %>%
  summarise(SW_test_result3 = list(shapiro.test(Refs)))

# Displaying the results
sw_results_Refs_GorP$SW_test_result3[[1]]
sw_results_Refs_GorP$SW_test_result3[[2]]
```

-   For both groups, the Shapiro-Wilks test for normality indicates that we should reject the null hypothesis that the data is normally distributed. Therefore, we will move right ahead with a Mann-Whitney U test, skipping Levene's test for homogeneity of variances.

2.  Conducting the Mann-Whitney U test.

```{r}
# To conduct Mann-Whitney U test
hyp_two_a_mann_test <- wilcox.test(Refs ~ Genuine_or_Fraudulent, data = data)

# Displaying the results
hyp_two_a_mann_test
```

-   The results of the Mann-Whitney U test indicate that we should accept the null hypothesis that the medians of the two groups are different. Although we mentioned in the data exploration section that we might want to remove the outlier before testing hypotheses with references as the outcome variable, the Mann-Whitney U test does not use means and should therefore not be biased due to outliers leverage on the statistics used. Therefore, it is not necessary to test this with the outlier removed.

#### **Testing Hypothesis 2b**

To test **Hypothesis 2b** we will conduct a one-way analysis of variance (ANOVA) to compare group means of `Refs` for SAFP, MAFP, SAGP, and MAGP. That is, we will determine whether SAFP contains more linguistic obfuscation than the genuine paper groups but less linguistic obfuscation than the MAFP. As references are technically count, rather than continuous, data, there is a higher likelihood that it will violate the assumption of normality, so in this case we will instead run a Kruskal-Wallis test to compare the groups by their median `Refs`.

1.  To check the assumption of normality we will use the `ggplot2` package to create Q-Q plots for `Refs`. The Q-Q plots will be investigated visually. If the assumption of normality is violated we will use the non-parametric Kruskal-Wallis test instead.

```{r}
# To produce Q-Q plots for Refs within the PaperType groups
ggplot(data, aes(sample = Refs)) + 
  stat_qq() + 
  facet_wrap(~ PaperType) +
  geom_qq_line()
```

-   Again, the data do not look like they are normally distributed based on the QQ-plots, especially for SAGP. To test this formally, we conduct Shapiro-Wilks tests for normality within groups.

```{r}
# Performing the Shapiro-Wilks test for normality within each group
sw_results_Refs_PT <- data %>%
  group_by(PaperType) %>%
  summarise(SW_test_result4 = list(shapiro.test(Refs)))

# Displaying the results
sw_results_Refs_PT$SW_test_result4[[1]]
sw_results_Refs_PT$SW_test_result4[[2]]
sw_results_Refs_PT$SW_test_result4[[3]]
sw_results_Refs_PT$SW_test_result4[[4]]
```

-   Indeed, the Shapiro-Wilks tests (except for the MAFP group) indicate that we should reject the null hypothesis that the data are normally distributed. We will now skip past testing for homogeneity of variances to the Kruskal-Wallis test as a robust version of a one-way ANOVA to test if there are any differences between group medians for references.

2.  Now to conduct the Kruskal-Wallis test.

```{r}
# To conduct Kruskal-Wallis
hyp_two_b_krusk <- kruskal.test(Refs ~ PaperType, data = data)

# To print the results of the Kruskal-Wallis test
hyp_two_b_krusk
```

-   The Kruskal-Wallis test indicates that we should accept the null hypothesis that there are no differences between the median `Refs` across groups. Similarly to **Hypothesis 2a**, this test should be robust to outliers, so there is no need to remove them here.

### **Hypothesis 3: Certainty**

**Hypothesis 3** investigates the use of certainty language in cases of scientific fraud. While a case study of Deidrik Stapel tended to use more certain language [@markowitz2014], others have found less certainty language in retracted papers than non-retracted papers [@dehdarirad2023]. Thus, **Hypothesis 3** is meant to provide clarity regarding the use of certainty language in scientific fraud. Specifically, it is stated as follows:

**Hypothesis 3:** Fraudulent research will contain less certainty than non-fraudulent research. \[Replication\]

#### **Testing Hypothesis 3:**

To test **Hypothesis 3**, we will compare the mean `CertSent` score between the `FPaper` and `GPaper` groups using an independent-samples t-test.

1.  To check the assumption of normality we will use the `ggplot2` package to create Q-Q plots for `CertSent` within the `FPaper` and `GPaper` groups. The Q-Q plots will be investigated visually. If the assumption of normality is violated we will use a bootstrapped t-test to fit a more robust model.

```{r}
# To produce Q-Q plots for CertSent within the FPaper and GPaper groups
ggplot(data, aes(sample = CertSent)) + 
  stat_qq() + 
  facet_wrap(~ Genuine_or_Fraudulent) +
  geom_qq_line()
```

-   The QQ-plots seem to diverge from normality in the middle of the distribution (for fraudulent papers) and at the lower end of the distribution (for the genuine papers). I will assess non-normality more formally using Shapiro-Wilks tests within groups.

```{r}
# Performing the Shapiro-Wilks test for normality within each group
sw_results_CertSent_GorF <- data %>%
  group_by(Genuine_or_Fraudulent) %>%
  summarise(SW_test_result5 = list(shapiro.test(CertSent)))

# Displaying the results
sw_results_CertSent_GorF$SW_test_result5[[1]]
sw_results_CertSent_GorF$SW_test_result5[[2]]
```

-   The results from the Shapiro-Wilks test for normality indicate that we should accept the null hypothesis that the data are normally distributed within groups.

-   Thus, we may assume normality for our t-test.

2.  To check the assumption of homogeneity of variances, we will conduct Levene’s test using the `car` package.

```{r}
# To load the car package for Levene's test
library(car)

# To conduct Levene's test for homogeneity of variances
hyp_three_levene_test <- leveneTest(CertSent ~ Genuine_or_Fraudulent, data = data)

# To display the Levene's test result
hyp_three_levene_test
```

-   Levene's test for homogeneity of variance indicates that we should reject the null hypothesis that the variances of the two groups is equal. Thus, we will run a Welch's t-test, which does not assume equal variances.

3.  Conducting the Welch's t-test.

```{r}
# To conduct Welch's t-test
hyp_three_welch_t_test <- t.test(CertSent ~ Genuine_or_Fraudulent, data = data, var.equal = FALSE)

# To print the Welch's t-test result
hyp_three_welch_t_test
```

-   Contrary to our hypothesis, Fraudulent Papers actually have higher mean certainty sentiment than Genuine Papers, although this result is not significant.

    -   These results align with [@dehdarirad2023].

### **Hypothesis 4:**

**Hypothesis 4** is a logical extension of the notion that to hide information you must first have control over related information flow. Based on this principle, **Hypothesis 4** is formulated as follows:

**Hypothesis 4:** For multi-author fraudulent papers, the fraudulent author will be the corresponding author more frequently than other authors. \[Novel\]

**Hypothesis 4** rests on the following two assumptions:

-   *First, all authors on a paper are equally likely to be the corresponding author.* Although this assumption oversimplifies norms of assigning scientists to be corresponding authors, in the absence of more information regarding, for example, author order, author responsibilities, or laboratory status, it is the most accurate prediction we can make regarding the likelihood of an author being the corresponding author. That is, in the absence of other information it is the baseline prediction.

-   *Second, if fraudulent authors to not attempt to control information flow, they are equally likely to be the corresponding as their coauthors are.* This may also oversimplify the norms of assigning scientists to be corresponding authors. For example, it is possible that fraudulent authors perform more data management, and it is possible that scientists that are responsible for data management are more likely to be corresponding authors. However, given the absence of this information (e.g., regarding research group norms), this assumption is reasonable.

These two assumptions allow us to make a prediction regarding the baseline expected frequency that fraudulent authors will be the corresponding authors of fraudulent research papers if they do not attempt to control information flow.

#### **Testing Hypothesis 4:**

To test **Hypothesis 4** we will conduct a binomial test to compare the observed likelihood that the fraudulent author is the corresponding author (i.e., `FraudCorrAuth`) to the expected probability given the assumption of equal likelihood for all authors and the average number of authors on each MAFP.

1.  To attain the expected probabiltiy we will calculate the inverse of the mean number of authors on all MAFP where the corresponding author is known. To do this, we will first need to filter out other  `PaperType` categories and MAFP cases where the corresponding author is unknown (i.e., `FraudCorrAuth` has a missing value) to make a new data frame, `hypothesis_four_df`. Then, we will calculate the mean `NumAuth` within this data frame and take its inverse.

```{r}
# To create data frame that only includes MAFP with a score for FraudCorrAuth
hypothesis_four_df <- data[data$PaperType == 'MAFP' & data$FraudCorrAuth %in% c("0", "1"), ]

# To calculate the mean number of authors within the data frame
mean_MAFP_NumAuth_corrauth_known <- mean(hypothesis_four_df$NumAuth)

# To display the mean value
mean_MAFP_NumAuth_corrauth_known

# To calculate the inverse of mean_MAFP_NumAuth_corrauth_known
expected_prob_FraudCorrAuth <- 1 / mean_MAFP_NumAuth_corrauth_known

# To display the expected probability
expected_prob_FraudCorrAuth
```

-   The mean number of authors in this subsample is 3.125, and the expected likelihood that the fraudulent author would be the corresponding author is therefore .32.

2.  Next, we will conduct a binomial test to compare the observed likelihood that a corresponding author is the fraudulent author to the expected probability (`expected_prob_FraudCorrAuth`).

```{r}
# To run the binomial test with the expected probability within the hypothesis four data frame
hyp_four_binomial_test <- binom.test(sum(hypothesis_four_df$FraudCorrAuth), nrow(hypothesis_four_df), p = expected_prob_FraudCorrAuth, alternative = "two.sided")

# To print the binomial test results
hyp_four_binomial_test
```

-   Although our observed likelihood is slightly higher than our expected probability, the hypothesis test indicates that we should accept the null hypothesis that there is no difference between them.

    -   Of course, this test has very low power, with only 8 trials.

# Saving Data

Now I will quickly export the main data frame to a csv file.

```{r}
write.csv(data, "post_analysis_study_dataset.csv")
```

The data is now available as "post_analysis_study_dataset.csv".

# Follow Up Analysis

This portion of the analysis was not preregistered. However, as Dr. Arbeau points out, the word count of papers may be a confound for our analyses assessing `Refs` between groups. To assess this possibility, we will create a new variable of the ratio of references to word count in a paper, and we will use this as our outcome variable to re-test **Hypothesis 2a** and **Hypothesis 2b**. Although both the Lexical Suite and Linguistic Inquiry and Word Count (LIWC) have produced word count variables in our dataset, will use the word count from the LIWC (i.e., `WC`), as this was the word count variable used to assess descriptive statistics.

## Retesting Hypothesis 2a

To retest **Hypothesis 2a**, we will first compute a new variable `RefWordRatio` by dividing `Refs` by `WC`. Then we will compare the mean `RefWordRatio` between the `FPaper` and `GPaper` groups using an independent-samples t-test.

1.  Computing the `RefWordRatio`.

```{r}
data$RefWordRatio <- (data$Refs/data$WC)
```

2.  Computing descriptive statistics for `RefWordRatio` both in the whole dataset and between `FPaper` and `GPaper` groups.

```{r}
RefWordRatio_ds <- describe(data$RefWordRatio)
RefWordRatio_ds

RefWordRatio_ds_by_GorF <- describeBy(data$RefWordRatio, group = data$Genuine_or_Fraudulent)
RefWordRatio_ds_by_GorF
```

-   In whole dataset:

    -   Interestingly, the skew statistic is lower for the `RefWordRatio` variable compared to the `Refs` variable alone (see [Descriptive Statistics]).

-   Within groups:

    -   The skew statistic seems to be higher in the genuine paper group. We will assess this with QQ-plots.

2.  Testing assumptions:
    -   First, we will assess normality with QQ-plots within groups.

```{r}
# Constructing QQ-plots within groups
ggplot(data, aes(sample = RefWordRatio)) + 
  stat_qq() + 
  facet_wrap(~ Genuine_or_Fraudulent) +
  geom_qq_line()
```

-   The data for both groups is not very tight to the theoretical values. We should take a look at the Shapiro-Wilk test.

```{r}
# Performing the Shapiro-Wilk test for normality within each group
sw_results_RefWordRatio_GorP <- data %>%
  group_by(Genuine_or_Fraudulent) %>%
  summarise(SW_test_result6 = list(shapiro.test(RefWordRatio)))

# Displaying the results
sw_results_RefWordRatio_GorP$SW_test_result6[[1]]
sw_results_RefWordRatio_GorP$SW_test_result6[[2]]
```

-   The results of the Shapiro-Wilk test indicate that we should reject the null hypothesis that the data are normally distributed for either group. Therefore, we will conduct a bootstrapped t-test to compare the two groups using an empirically determined, robust confidence interval. This will also allow us to skip the test for homogeneity of variance.

3.  Conducting the bootstrapped t-test.

```{r}
# Loading the boot package for bootstrapping
library(boot)

# Setting the seed
set.seed(616913)

# Defining the statistic function for bootstrapping
t_stat_boot_RefWCRatio <- function(data, index) {
  resampled_data_RefWCRatio <- data[index, ] # Resample the data with replacement
  t_result <- t.test(RefWordRatio ~ Genuine_or_Fraudulent, data=resampled_data_RefWCRatio)
  return(t_result$statistic)
}

# Performing the bootstrapping
boot_results_RefWCRatio <- boot(data = data, statistic = t_stat_boot_RefWCRatio, R = 1000)

# Performing the regular t-test
stud_t_test_RefWCRatio <- t.test(RefWordRatio ~ Genuine_or_Fraudulent, data = data)

# Calculating Cohen's d using the effsize package
d_RefWCRatio <- cohen.d(RefWordRatio ~ Genuine_or_Fraudulent, data = data)

# Calculate the 95% confidence interval from the bootstrapped results
boot_ci_RefWCRatio <- boot.ci(boot_results_RefWCRatio, type = "bca")

# Report the results
cat("Mean Difference: ", stud_t_test_RefWCRatio$estimate, "\n",
    "95% CI: ", boot_ci_RefWCRatio$bca[4], boot_ci_RefWCRatio$bca[5], "\n",
    "t-value: ", stud_t_test_RefWCRatio$statistic, "\n",
    "p-value: ", stud_t_test_RefWCRatio$p.value, "\n",
    "Cohen's d: ", d_RefWCRatio$estimate, "\n")
```

-   The results of the t-test with bootstrapped confidence intervals indicate that we should accept the null hypothesis that there is no difference between the two groups.

## Retesting Hypothesis 2b

To retest **Hypothesis 2b** we will compare the mean `RefWordRatio` between the `PaperType` groups using an analysis of variance, after computing descriptive statistics within `PaperTyped` groups.

1.  Computing descriptive statistics within `PaperType` groups.

```{r}
RefWordRatio_ds_by_PaperType <- describeBy(data$RefWordRatio, group = data$PaperType)
RefWordRatio_ds_by_PaperType
```

-   There is not much to say about these descriptives (particularly when they are so small that it is difficult to assess them).

2.  Testing assumptions:

-   First, we will assess normality with QQ-plots within groups.

```{r}
# To produce Q-Q plots for Refs within the PaperType groups
ggplot(data, aes(sample = RefWordRatio)) + 
  stat_qq() + 
  facet_wrap(~ PaperType) +
  geom_qq_line()
```

-   The SAGP group looks like it deviated from normality towards the bottom of the distribution, so we will be performing the Shapiro-Wilk test for normality within PaperType groups.

```{r}
# Performing the Shapiro-Wilk test for normality within each group
sw_results_RefWordRatio_PT <- data %>%
  group_by(PaperType) %>%
  summarise(SW_test_result7 = list(shapiro.test(RefWordRatio)))

# Displaying the results
sw_results_RefWordRatio_PT$SW_test_result7[[1]]
sw_results_RefWordRatio_PT$SW_test_result7[[2]]
sw_results_RefWordRatio_PT$SW_test_result7[[3]]
sw_results_RefWordRatio_PT$SW_test_result7[[4]]
```

-   Although for most of the groups we can accept the null hypothesis of normally distributed data, the results for the SAGP indicate that we should reject the null. Therefore, we will skip the assumption of homogeneity of variance and conduct a one-way ANOVA with bootstrapped confidence intervals for the F-statistic.

3.  Conducting the bootstrapped ANOVA.

```{r}
# Setting the seed
set.seed(616913)

# Define the statistic function for bootstrapping ANOVA
f_stat_boot_RefWCRatio <- function(data, index) {
  resampled_data <- data[index, ] # Resample the data with replacement
  aov_result <- aov(RefWordRatio ~ PaperType, data=resampled_data)
  return(summary(aov_result)[[1]]$F[1]) # Extract the F-statistic
}

# Perform the bootstrapping
boot_results_f_RefWCRatio <- boot(data = data, statistic = f_stat_boot_RefWCRatio, R = 1000)

# Perform and print the original ANOVA to get the original F-statistic and p-value
original_aov_RefWCRatio <- aov(RefWordRatio ~ PaperType, data=data)
summary(original_aov_RefWCRatio)

# Calculate and report the bootstrapped confidence interval for the F-statistic
boot_f_ci_RefWCRatio <- boot.ci(boot_results_f_RefWCRatio, type="bca")
cat("Bootstrapped 95% Confidence Interval for F-statistic: ", boot_f_ci_RefWCRatio$bca[4], " to ", boot_f_ci_RefWCRatio$bca[5], "\n")
```

-   The results of the ANOVA and the fact that the observed F-statistic is within the bootstrapped confidence interval suggest that we should accept the null hypothesis that there is no difference between the means of the `PaperType` groups on the ratio of `Refs` to `WC`.

# Saving Data

```{r}
write.csv(data, "post_followup_analyses_study_dataset.csv")
```
