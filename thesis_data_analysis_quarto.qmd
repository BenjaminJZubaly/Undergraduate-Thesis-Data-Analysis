---
title: "Thesis Data Analysis: The Psychology of Scientific Fraud"
author: "Benjamin Zubaly"
date: March 4, 2024
format:
  html:
    self-contained: true
    toc: true
  docx:
    toc: true
  pdf:
    toc: true
editor: visual
csl: apa.csl
bibliography: references.yaml
---

```{r setup, include=FALSE}
# Set CRAN mirror
options(repos = list(CRAN = "https://cloud.r-project.org/"))
```

# Introduction

This document is intended to track the data analysis for my undergraduate thesis project on the psychology of scientific fraud. I have already planned out (and [preregistered](https://osf.io/vmq48/?view_only=5d55e73ffc7948748e5a7049c6e52161 "OSF Preregistration Link")) my data analysis plan (see [here](https://benjaminjzubaly.github.io/The-Psychology-of-Scientific-Fraud-Thesis/#data-analysis "Thesis Data Analysis Plan")), and I will note throughout the document if I deviate from this plan and why. This project is also being tracked in a private GitHub repository in case I need to revert back to a previous version of the project due to a fatal error.

## Overview of Sections

-   **Data Cleaning:** The data cleaning section will take the finalized dataset that I have included in the project directory and use it to create the remaining variables that we need for our analysis. We will also save this dataset.

-   **Data Exploration:** In the data exploration section, we will check for and deal with missing data, run descriptive statistics of our outcome variables, visualize our data, and run bivariate correlations.

-   **Testing Hypotheses:** In the hypothesis testing section, we will test each of our hypotheses one-by-one, according to our analysis plan.

## Directory Set-Up

In order to ensure that this analysis is computationally reproducible, I have included everything that is needed to complete this analysis (just the finalized data file, "study_dataset.csv") in the current working directory. The code will be displayed before the output of each analysis.

## Variable Definitions

Although the following variables may not exist until after the data cleaning section, here is what each variable name refers to, so that you can refer to them while examining the code.

`DOI`: Unique identifier for each paper (i.e., the paper DOI).

`PaperType`: Categorical variable indicating SAFP, SAGP, MAFP, or MAGP.

`LingObf`: Continuous variable for linguistic obfuscation.

`CertSent`: Continuous variable for certainty sentiment.

`Refs`: Count variable for references.

`FraudCorrAuth`: Dichotomous variable indicating if the fraudulent author is the corresponding author (1) or is not (0). Unknown cases will be marked in a seperate variable with this variable left blank.

`NumAuth`: Count variable indicating the number of authors for each paper.

`abstraction`: Abstraction index composed of the sum of standardized scores for `article`, `prep`, and `quantity`.

`article`: Articles from LIWC.

`prep`: Prepositions from LIWC.

`quantity`: Quantities from LIWC.

`cause`: Causation terms from LIWC.

`jargon`: The percent of words not captured by LIWC (100-`Dic`).

`Dic`: The percentage of words captured by all LIWC dictionaries.

`emo_pos`: Positive emotion terms from LIWC.

`flesch_re`: Flesch Reading Ease from ARTE.

## Initial Package Installations

If you would like to reproduce this analysis, here are the package I will be using, so that they can be cued before starting.

```{r}
install.packages("readr")       # For reading data
install.packages("dplyr")       # For data manipulation and handling of missing data
install.packages("psych")       # For descriptive statistics, correlations
install.packages("ggplot2")     # For data visualization
install.packages("car")         # For diagnostic tests such as Levene's test
install.packages("rcompanion")  # For Games-Howell post-hoc test (if applicable)
install.packages("dunn.test")   # For Dunn post-hoc test (if applicable)
```

## Initial Import of Data

We will not load in the dataset "study_dataset.csv" from the working directory.

```{r}
library(readr) # Loading the readr package

data <- read_csv("study_dataset.csv") # Loading in study dataset as "data"
```

I have viewed the data frame, and the data seems to have loaded correctly.

# Data Cleaning

To conduct the analysis, we will first need to calculate the `LingObf` variable by calculating the `abstraction` index and `jargon` words; creating standardized scores for `abstraction`, `cause`, `jargon`, `emo_pos`, and `flesch_re`; and calculating the `LingObf` composite variable from these standardized scores.

1.  First, we will calculate the `abstraction` index by creating standardized scores for `article`, `prep`, and `quantity` and summing them.

```{r}
# Calculate standardized scores for article, prep, and quantity and add them to the dataset
data$articles_standardized <- scale(data$article)
data$prep_standardized <- scale(data$prep)
data$quantity_standardized <- scale(data$quantity)

# Create the new variable 'abstraction' as the sum of the three standardized variables
data$abstraction <- (data$articles_standardized + data$prep_standardized + data$quantity_standardized)
```

-   After viewing the data, the transformations and variable calculation seem to have occurred appropriately.

2.  Next, we will calculate the `jargon` words by subtracting `Dic` from 100.

```{r}
# Calculate the new variable 'jargon' by subtracting 'Dic' from 100
data$jargon <- (100 - data$Dic)
```

-   After viewing the data, the variable calculation seem to have occurred appropriately.

3.  Next, we will create standardized scores for each subcomponent of the `LingObf`.

```{r}
# Standardize the new set of variables and add them to the dataset
data$abstraction_standardized <- scale(data$abstraction)
data$cause_standardized <- scale(data$cause)
data$jargon_standardized <- scale(data$jargon)
data$emo_pos_standardized <- scale(data$emo_pos)
data$flesch_re_standardized <- scale(data$flesch_re)
```

-   After viewing the data, the variable transformations seem to have occurred appropriately.

4.  Now we will calculate the `LingObf` variable using the following formula: \[cause_standardized + abstraction_standardized + jargon_standardized\] – \[emo_pos_standardized + flesch_re_standardized\].

```{r}
# Calculate 'LingObf'
data$LingObf <- (data$cause_standardized + data$abstraction_standardized + data$jargon_standardized) - (data$emo_pos_standardized + data$flesch_re_standardized)
```

-   After viewing the data, the variable calculation seem to have occurred appropriately.

5.  Lastly, our variable that indicates certainty sentiment is currently `certainty_avg`, but to make things easier I am going to copy this data into a new variables called `CertSent`.

```{r}
data$CertSent <- data$certainty_avg
```

To ensure that our clean data is saved, we will write the dataset to the current working directory.

```{r}
# Writing our data as a csv file in the current working directory
write.csv(data, "clean_study_data.csv")
```

-   I have opened the saved data file outside of Rstudio, and it seems to have been written correctly.

# Data Exploration

## Missing Data

1.  Data will be first inspected for missing scores.

```{r}
library(dplyr) # Loading the dplyr package for data manipulation and handling missing values

# To summarize the number of missing values in each column
missing_data_summary <- sapply(data, function(x) sum(is.na(x)))

print(missing_data_summary) # To see summary of missing values for all columns
```

-   Taking a look at our outcome variables, there are no missing scores, so we do not need to impute any values.

## Descriptive Statistics

1.  I will now generate descriptive statistics for each relevant variable in the dataset. I originally (in the preregistered plan) was simply going to deploy the `describe()` function on the entire dataset, but because I retained all of the columns from the Retraction Watch Database [@retracti2023] and all of the output from the text analysis packages [@aggarwal2022; @boyd2022; @rocklage2023] there are currently 219 variables. Because this would be unmanageable, I am going to only calculate descriptive statistics for a selection of variables of interest. I will first create a dataframe with only the continuous variables that I am interested in generating descriptive statistics for, and I will use the `psych` package to produce the descriptive statistics for these variables.

```{r}
library(psych) # Loading the psych package

# Selecting the continuous variables I am interested in getting descriptive statistics for (plus paper type for the next part)
continuous_data_for_descriptives <- data[c("year", "Refs", "flesch_re", "WC", "abstraction", "jargon", "CertSent", "LingObf", "cause", "emo_pos", "article", "prep", "quantity", "PaperType")]

# Generating descriptive statistics for variables of interest
continuous_descriptive_stats_all <- describe(continuous_data_for_descriptives)

# Displaying the results of the descriptive stats for the variables listed above
continuous_descriptive_stats_all
```

-   I won't make too many comments here, because for most of these measures there are no formal or informal norms against which to judge them. That being said:
    -   **Year:** The mean year is mid-2009, with a standard deviation of 10 years (max 2022 and min 1980), indicating that the sample is recent enough to be relevant but also spans quite a number of years. This I think is good insofar as the recent history of academic publishing is represented more fully (some recent investigations limited their search to the three years prior to publication). There is some negative skew, which I suspect is due to the 1980 paper being quite a bit older than most papers.
    -   **Refs:** The mean number of references is 45.82 with a standard deviation of 23.47. At least intuitively, this seems like a pretty standard distribution of references if we were to randomly select papers from the literature. However, the range is huge, with one paper showing 146 references—perhaps why the skew is positive. This may be something to consider when making group comparisons, as this point may have significant leverage.
    -   **WC:** The average word count is 4436.25, and the standard deviation is 2248.24. It is also positively skewed, and although this should not affect our linguistic dependent variables it may be a confounding factor for references. Indeed, taking a look at the dataset I can see that the second highest value for word count (9670) also has the maximum value for references (the outlier noted above at 146 references). This will be something to watch out for.
-   Now we will create descriptive statistics for each of the continuous variables above within the `PaperType` groups.

```{r}
# Making PaperType a factor variable in the continuous variable dataframe to allow for grouping
continuous_data_for_descriptives$PaperType <- factor(continuous_data_for_descriptives$PaperType, levels = c("SAFP", "MAFP", "SAGP", "MAGP"), labels = c("Single-Authored Fraudulent Papers", "Multi-Authored Fraudulent Papers", "Single-Authored Genuine Papers", "Multi-Authored Genuine Papers"))

# Generating descriptive statistics within PaperType groups
descriptive_stats_by_PaperType <- describeBy(continuous_data_for_descriptives, group = continuous_data_for_descriptives$PaperType)

descriptive_stats_by_PaperType
```

-   Some quick notes:

    -   **Year:** The year seems to be relatively similar between groups, with the single-author groups (SAGP and SAFP) both having mid-2008 as the mean and the multi-author groups both having mid-2010 as the mean. This may be partially due to there being single outliers (matched papers that are both outliers) at the lower end of the distribution.

    -   **Refs:** As I suspected, the outlier of 146 seems to be pulling the mean for SAGP up to 51.05 (the means for all other groups are around 44). This is corroborated by the skew statistics which show that only the SAGP group has a skew value above 1. I may run the analyses where references is the outcome variable both with and without this case in the model, or I may use word count as a covariate in the model to try to account for it.

    -   **WC:** For some reason, there seems to be a notable difference in mean word count between the single-authored fraudulent group and the rest of the groups. I could speculate as to why this is the case, but I may do an unplanned follow-up analysis of this.

2.  Frequencies will now be produced for categorical variables, both for the data in general and within `PaperType` groups.

-   First, we will make the variables `inst_pres`, `gender`, `simple_reason`, `Country`, and `PaperType` factor variables.

```{r}
# Changing inst_pres (institutional prestige), gender, simple_reason, Country, and PaperType into factor variables
data$inst_pres <- factor(data$inst_pres, levels = c(0, 1), labels = c("Not Major Research Institution", "Major Research Institution"))
data$gender <- factor(data$gender, levels = c("FEMALE", "MALE"), labels = c("Female", "Male"))
data$simple_reason <- factor(data$simple_reason, levels = c("f_data", "f_image", "m_image", "f_data f_image"), labels = c("Fabricated/Falsified Data", "Fabricated/Falsified Image", "Manipulated Image", "Fabricated/Falsified Data and Image"))
data$Country <- factor(data$Country)
data$PaperType <- factor(data$PaperType)
```

-   Next, we will produce frequency tables for each categorical variable of interest for the whole dataset.

```{r}
# Creating the frequency tables for each categorical variable
freq_tab_inst_pres <- table(data$inst_pres)
freq_tab_gender <- table(data$gender)
freq_tab_simple_reason <- table(data$simple_reason)
freq_tab_Country <- table(data$Country)
freq_tab_PaperType <- table(data$PaperType)

# Displaying the frequency tables
freq_tab_inst_pres
freq_tab_gender
freq_tab_simple_reason
freq_tab_Country
freq_tab_PaperType
```

-   Let's take a look at the way these are split:

    -   First, there seems to ba a preponderance of lower status (not major) research institutions in the whole dataset (70:18).

    -   Next, most of the papers were retracted due to fabricated/falsified data (*n* = 27), followed by those that manipulated images (*n* = 12), fabricated/falsified images (*n* = 3), or both fabricated/falsified data and images (*n* = 2).

    -   For country, the most common category of papers seem to be from the United States (*n* = 29), followed by China and India (both *n* = 10) and the Netherlands (*n* = 6), with no other countries totaling more than *n* = 4.

    -   Finally, we can see that the papers are evenly split between each of our groups (*n* = 22; *N* = 88).

-   Next, we will produce the frequencies within `PaperType` groups. Because we were not able to perfectly match across our matching characteristics, we will take this opportunity to look at how far off we were. A proportion table will be produced to more easily compare frequencies across `PaperType` groups. First, however, we need to make the matching dummy variables into factor variables with labels to make them more interpretable.

```{r}
# Creating factor variables out of the dummy variables that track matching across groups
data$different_country <- factor(data$different_country, levels = c(0, 1), labels = c("Same Country", "Different Country"))
data$different_gender <- factor(data$different_gender, levels = c(0, 1), labels = c("Same Gender", "Different Gender"))
data$different_inst_pres <- factor(data$different_inst_pres, levels = c(0, 1), labels = c("Same Institutional Prestige", "Different Institutional Prestige"))
data$different_journal <- factor(data$different_journal, levels = c(0, 1), labels = c("Same Journal", "Different Journal"))
```

-   I have checked the data set, and the labels seem to have been assigned correctly. Now we will produce the frequency tables and proportion tables.
    -   Because we are taking a look at matching characteristics here, I will also take a look at the `year_difference` (the difference between the years of publication for the matched papers) variable within each group by using the `describe()` function from the `psych` package (like for the continuous variables above). In addition, I will take a look at the `NumAuth` (number of authors) for each of the groups.

```{r}
# Frequency tables and proportion tables
  # institutional prestige
frequency_table_by_PaperType_inst_pres <- table(data$PaperType, data$inst_pres)
frequency_table_by_PaperType_inst_pres
proportion_table_by_PaperType_inst_pres <- prop.table(frequency_table_by_PaperType_inst_pres, margin = 1)
proportion_table_by_PaperType_inst_pres
  # gender
frequency_table_by_PaperType_gender <- table(data$PaperType, data$gender)
frequency_table_by_PaperType_gender
proportion_table_by_PaperType_gender <- prop.table(frequency_table_by_PaperType_gender, margin = 1)
proportion_table_by_PaperType_gender
  # reason
frequency_table_by_PaperType_reason <- table(data$PaperType, data$simple_reason)
frequency_table_by_PaperType_reason
proportion_table_by_PaperType_reason <- prop.table(frequency_table_by_PaperType_reason, margin = 1)
proportion_table_by_PaperType_reason
  # country
frequency_table_by_PaperType_country <- table(data$PaperType, data$Country)
frequency_table_by_PaperType_country
proportion_table_by_PaperType_country <- prop.table(frequency_table_by_PaperType_country, margin = 1)
proportion_table_by_PaperType_country

# And for the matching characteristic dummy variables
  # Country different
frequency_table_by_PaperType_different_country <- table(data$PaperType, data$different_country)
frequency_table_by_PaperType_different_country
proportion_table_by_PaperType_different_country <- prop.table(frequency_table_by_PaperType_different_country, margin = 1)
proportion_table_by_PaperType_different_country
  # Gender different
frequency_table_by_PaperType_different_gender <- table(data$PaperType, data$different_gender)
frequency_table_by_PaperType_different_gender
proportion_table_by_PaperType_different_gender <- prop.table(frequency_table_by_PaperType_different_gender, margin = 1)
proportion_table_by_PaperType_different_gender
  # Institutional prestige different
frequency_table_by_PaperType_different_inst_pres <- table(data$PaperType, data$different_inst_pres)
frequency_table_by_PaperType_different_inst_pres
proportion_table_by_PaperType_different_inst_pres <- prop.table(frequency_table_by_PaperType_different_inst_pres, margin = 1)
proportion_table_by_PaperType_different_inst_pres
  # Journal different
frequency_table_by_PaperType_different_journal <- table(data$PaperType, data$different_journal)
frequency_table_by_PaperType_different_journal
proportion_table_by_PaperType_different_journal <- prop.table(frequency_table_by_PaperType_different_journal, margin = 1)
proportion_table_by_PaperType_different_journal

# Descriptive statistics for year off within PaperType groups
year_difference_descriptives <- describeBy(data$year_difference, group = data$PaperType, na.rm = TRUE)
year_difference_descriptives
num_auth_descriptives <- describeBy(data$NumAuth, group = data$PaperType, na.rm = TRUE)
num_auth_descriptives
```

-   **Overview of Categorical Variables:**

    -   **Institutional Prestige:** The split for frequencies/proportion of is very close to even across groups. The MAFP and SAGP groups have 17 papers from major research institutions and 5 from lower status research institutions (.77/.22), and the MAGP and SAFP groups have 18 papers from major research institutions and 4 from lower status research institutions (.82/.18).

    -   **Gender:** The split between groups is also very similar for gender, with MAFP, MAGP, and SAGP groups each having 4 female first authors and 18 male first authors (.18/.82). SAFP had 3 female first authors and 19 male first authors (.14/.86).

    -   **Reason (Not Matched Between Groups During Data Collection):** The reason for retraction is only relevant for the fraudulent paper groups, and there is a similar split between them. The SAFP had 15 papers in the group for Fabricated/Falsified Data, 2 paper for Fabricated/Falsified Images, 5 for Manipulated Images, and 0 that Fabricated/Falsified Data and Images (.68/.09/.22/.00). The MAFP had 12 papers in the group for Fabricated/Falsified Data, 1 paper for Fabricated/Falsified Images, 7 for Manipulated Images, and 2 for Fabricated/Falsified Data and Images (.55/.05/.32/.09).

    -   **Country:** Because there are many countries, I will not delineate them all here but rather refer you to the data printed above if you are interested in diving deeper. To highlight one thing, we can see that papers from the United States (our most common country in the whole data set) tend to be overrepresented in the frequencies for each group (MAFP, 8/.36; MAGP, 9/.40; SAFP, 7/.32; SAGP, 5/.23). There are no other countries with proportions that exceed the lowest proportion of United States papers in any group (all\<.23).

-   **Overview of Matching Characteristic Dummy Variables and Average Differences:**

    -   Although the frequencies and proportions of the matched variables within groups above indicates to some degree how similar each group is, the more important metric of how closely matched the groups are is between the groups for which comparisons are to be made. For this reason, I dummy coded whether each paper matched it's pair on each characteristic or not as a binary variable (which we labeled above) before loading the data. Because the SAFP papers were not gathered by matching to any group, there are no data for the SAFP group for any of the dummy variables, and journal only didn't match across some of the SAFP and MAFP pairs. Each of the other groups is in reference to the group against which it will be compared (i.e., data for MAFP represent how well they match SAFP papers, data for MAGP represent how well they match with MAFP, data for SAGP represent how well they match with SAFP). In addition, I coded how different the year was between papers, such that positive values indicate the number of years the original paper was published after the matched paper (negative values indicate the matched paper was published after the paper it was being matched to).

    -   **MAFP compared with SAFP:**

        -   Country was the same for 14 (.64) pairs and different for 8 (.36) pairs.

        -   Gender was the same for 18 (.82) pairs and different for 4 (.18) pairs.

        -   Institutional prestige was the same for 17 (.77) pairs and different for 5 (.23) pairs.

        -   Journal was the same for 11 (.50) pairs and different for 11 (.50) pairs.

        -   The average year difference was -1.36, indicating that the MAFP tended to be slightly more recent than the SAFP.

        -   Finally, the mean number of authors for MAFP is 3.73.

    -   **MAGP compared with MAFP:**

        -   Country was the same for 21 (.95) pairs and different for 1 (.05) pairs.

        -   Gender was the same for 20 (.91) pairs and different for 2 (.09) pairs.

        -   Institutional prestige was the same for 21 pairs (.95) and different for 1 (.05) pair.

        -   Journal was the same for every paper.

        -   The average year difference was -.64, indicating that the MAGP tended to be slightly more recent than the MAFP.

        -   Finally, the mean number of authors for MAFP is 4.05.

    -   **SAGP compared with SAFP:**

        -   Country was the same for 10 (.45) pairs and different for 12 (.55) pairs.

        -   Gender was the same for 19 (.86) pairs and different for 3 (.14) pairs.

        -   Institutional prestige was the same for 21 pairs (.95) and different for 1 (.05) pair.

        -   Journal was the same for every paper.

        -   The average year difference was 0, indicating that the SAGP and SAFP are exactly matched for the average year of publication.

-   **Summary of matching characteristics:**

    -   Looking at the data, the most difficult characteristic to match was country across all categories of papers, except for MAFP compared with SAFP, where journal was the most difficult to match. Nevertheless, for many of the situations where we were unable to match the specific paper, this was balanced out by not being able to match another paper in the opposite direction, as the frequencies within paper types demonstrates.

## Data Visualization

1.  Using the `ggplot2` package, frequency distributions and box plots will be generated for `LingObf`, `CertSent`, and `Refs`. A bar plot will be produced for `FraudCorrAuth`.

```{r}
library(ggplot2) # To load the ggplot2 package

# To produce histogram and box plot for LingObf
ggplot(data, aes(x = LingObf)) + 
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of Linguistic Obfuscation for Entire Dataset")
ggplot(data, aes(y = LingObf)) + 
  geom_boxplot(fill = "lightblue", color = "black") +
  labs(title = "Box Plot of Linguistic Obfuscation for Entire Dataset")

# To produce histogram and box plot for CertSent
ggplot(data, aes(x = CertSent)) + 
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of Certainty Sentiment for Entire Dataset")
ggplot(data, aes(y = CertSent)) + 
  geom_boxplot(fill = "lightblue", color = "black") +
  labs(title = "Box Plot of Certainty Sentiment for Entire Dataset")

# To produce histogram and box plot for Refs
ggplot(data, aes(x = Refs)) + 
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of References for Entire Dataset")
ggplot(data, aes(y = Refs)) + 
  geom_boxplot(fill = "lightblue", color = "black") +
  labs(title = "Box Plot of References for Entire Dataset")

# To produce bar plot for FraudCorrAuth
ggplot(data, aes(x = FraudCorrAuth)) + 
  geom_bar(fill = "coral") +
  labs(title = "Bar Plot of Fraudulent Corresponding Author for MAFP")
```

-   **LingObf:** Linguistic obfuscation seems to have a bit of a wonky distribution with a few outliers at the low end of the distribution (M = 0, SD = 1.67; from descriptives above). This can be observed in the box plot as well, which shows one of the particularly low values.

-   **CertSent:** Certainty sentiment does not seem to be skewed despite having an outlier at the low end of the distribution as well. The box plot highlights three values at the low end of the scale as being outliers.

-   **Refs:** References has a strange distribution, one that looks like it is actually composed of two latent classes. It also has two outlier values way at the high end of the distribution, as shown by the histogram and the box plot.

-   **FraudCorrAuth:** The fraudulent corresponding author variable shows an even split between papers for which the fraudulent author was and was not the corresponding author.

2.  Using the `ggplot2` package, frequency distributions and box plots will be generated for `LingObf`, `CertSent`, and `Refs` within `PaperType` groups.

```{r}
# To produce histogram and box plot for LingObf
ggplot(data, aes(x = LingObf)) + 
  geom_histogram(fill = "blue", color = "black") +
  facet_wrap(~ PaperType) +
  labs(title = "Histograms of Linguistic Obfuscation for Entire Dataset")
ggplot(data, aes(y = LingObf)) + 
  geom_boxplot(fill = "lightblue", color = "black") +
  facet_wrap(~ PaperType) +
  labs(title = "Box Plots of Linguistic Obfuscation Within Paper Type Groups")

# To produce histogram and box plot for CertSent
ggplot(data, aes(x = CertSent)) + 
  geom_histogram(fill = "blue", color = "black") +
  facet_wrap(~ PaperType) +
  labs(title = "Histograms of Certainty Sentiment Within Paper Type Groups")
ggplot(data, aes(y = CertSent)) + 
  geom_boxplot(fill = "lightblue", color = "black") +
  facet_wrap(~ PaperType) +
  labs(title = "Box Plots of Certainty Sentiment Within Paper Type Groups")

# To produce histogram and box plot for Refs
ggplot(data, aes(x = Refs)) + 
  geom_histogram(fill = "blue", color = "black") +
  facet_wrap(~ PaperType) +
  labs(title = "Histograms of References Within Paper Type Groups")
ggplot(data, aes(y = Refs)) + 
  geom_boxplot(fill = "lightblue", color = "black") +
  facet_wrap(~ PaperType) +
  labs(title = "Box Plots of References Within Paper Type Groups")
```

-   A few quick notes:

    -   **LingObf:**

        -   Positive outliers for linguistic obfuscation seem to be represented in the multi-author groups, and negative outliers seem to be in the single-author groups (although the lowest values do not show up on the SAFP box plot, perhaps because there are two of them that can be seen in the histogram which suggests that they may skew the spread statistics enough to envelop them within the whiskers).

    -   **CertSent:**

        -   The otulier at the low end of the certainty sentiment distribution seems to belong to the MAGP group.

    -   **Refs:**

        -   The positive outliers we have noted a couple times now for references seem to be in the SAGP group, corroborating our speculation above (see [Descriptive Statistics]).

        -   Strangely, we noted that the references seemed to have two latent classes, but, at least visually, this does not seem to represent any particular paper group, as the latent classes show up in the SAFP and MAGP groups (as well as perhaps the SAGP group). I am not sure why this is.

    -   Overall, the strange values are nto all within one group, which suggests that they are due to random variation rather than one paper that was not read appropriately by the text analysis programs (or some other form of systematic problem with one or two observations in the data set).

## Bivariate Correlations

In order to further explore data characteristics, bivariate correlations between outcome variables will be produced. As these are only for exploring data and not testing predictions, assumptions will not be tested, and the relationships will not be probed for statistical significance. Then, correlations for subcomponents of composite variables will be produced.

1.  First, Pearson bivariate correlations will be produced for `LingObf`, `CertSent`, and `Refs`.

```{r}
# To produce correlation matrix for LingObf, CertSent, and Refs variables within whole dataset
  # To calculate the Pearson correlation coefficients between the numerical outcome variables
correlation_matrix_num_outcomes <- cor(data[c("LingObf", "CertSent", "Refs")], use = "complete.obs", method = "pearson")
  # To display the correlation matrix for numerical outcome variables
correlation_matrix_num_outcomes
```

-   All of the variables here show negligible bivariate relationships.

2.  Next, here are the point-biserial correlations between `FraudCorrAuth` and `LingObf`, `CertSent`, and `Refs`.

```{r}
# Calculating Point-biserial correlations between FraudCorrAuth and numerical outcome variables
pbcorr_FraudCorrAuth_LingObf <- cor(data$FraudCorrAuth, data$LingObf, use = "complete.obs", method = "pearson")
pbcorr_FraudCorrAuth_CertSent <- cor(data$FraudCorrAuth, data$CertSent, use = "complete.obs", method = "pearson")
pbcorr_FraudCorrAuth_Refs <- cor(data$FraudCorrAuth, data$Refs, use = "complete.obs", method = "pearson")

# Displaying the correlation coefficients
pbcorr_FraudCorrAuth_LingObf
pbcorr_FraudCorrAuth_CertSent
pbcorr_FraudCorrAuth_Refs
```

-   While the relationship between FraudCorrAuth and LingObf is large (r = .55) and the relationship between FraudCorrAuth and CertSent is very large (r = .83), I will refrain from interpreting these because the sample is severely underpowered with a mere *n* = 8 observations.

3.  Now, we will produce bivariate correlations for the subcomponents of the `abstraction` index (see [Data Cleaning] for more details.

```{r}
# Calculate pairwise correlations among the subcomponents of the abstraction index
correlation_matrix_abstraction_subcomponents <- cor(data[,c("article", "prep", "quantity")])

# Display the correlation matrix
correlation_matrix_abstraction_subcomponents
```

-   The correlations here (.03 \< r \< .09) are quite a bit smaller (i.e., nonexistent) than @markowitz2016 found for their sample when they constructed the abstraction index (.176 \< r \< .263). This calls into question the reliability of the abstraction index as a subcomponent of the linguistic obfuscation index in our sample, and it calls into question the validity of each of the indicators of the abstraction index (articles, prepositions, and quantity words) in our sample.

4.  Now we will produce Pearson bivariate correlations for the subcomponents of the `LingObf` index.

```{r}
# Calculate pairwise correlations among the subcomponents of LingObf
correlation_matrix_LingObf_subcomponents <- cor(data[,c("cause", "abstraction", "jargon", "emo_pos", "flesch_re")])

# Display the correlation matrix
correlation_matrix_LingObf_subcomponents
```

-   If we let the Linguistic Obfuscation Index be *LFI*, causal terms be *C*, the abstraction index be *A*, jargon be *J*, positive emotion terms be *P*, and Flesch reading ease be *F*, the mathematical model for the construction of the Linguistic Obfuscation index for an observation *i* can be stated as follows:

$$
LFI_i = (C_i + A_i + J_i) - (P_i + F_i)
$$

-   If this model is internally consistent, we should expect that each of the variables within the aggregated variables (i.e., that are within parentheses with one another) should be positively correlate. We should also expect that variables will be negatively correlated across the aggregates (i.e., each of the variables that are in opposite aggregates.

    -   Expected Positive Correlations:

        -   We should expect the relationships between causal terms, the abstraction index, and jargon words to be positive. However, the relationships all seem to be (small) negative relationships.

        -   We also should expect the relationship between positive emotion words and Flesch reading ease to be positive, but it is also a small, negative correlation.

    -   Expected Negative Correlations:

        -   As we should expect, there is a moderate negative relationship between causal terms and Flesch reading ease.

        -   However, we should expect the relationship between causal terms and positive emotion terms to be negative, but it is a small, positive correlation.

        -   We should expect a negative relationship between the abstraction index and positive emotion terms, but there is no relationship to speak of.

        -   We also should expect a negative relationship between the abstraction index and Flesch reading ease, but there is a moderate positive correlation.

        -   As should expect, there is a small negative relationship between Jargon and positive emotion terms.

        -   Lastly, we should expect a negative correlation between jargon and Flesch reading ease, but there is a small, positive correlation between them.

    -   The observed relationships here may be expected *a priori* in some cases (e.g., the relationship between jargon and Flesch reading ease may be expected to be a positive relationship because the longer sentences someone writes the more likely they may be to use nonstandard words, simply by volume). However, from an internal consistency perspective, most of the relationships (8/10) we have observed here are either in the wrong direction or are too small to be considered.

# Testing Hypotheses
