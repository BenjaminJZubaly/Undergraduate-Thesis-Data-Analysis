---
title: "Thesis Data Analysis: The Psychology of Scientific Fraud"
author: "Benjamin Zubaly"
date: March 4, 2024
format:
  html:
    self-contained: true
    toc: true
  docx:
    toc: true
  pdf:
    toc: true
editor: visual
csl: apa.csl
bibliography: references.yaml
---

```{r setup, include=FALSE}
# Set CRAN mirror
options(repos = list(CRAN = "https://cloud.r-project.org/"))
```

# Introduction

This document is intended to track the data analysis for my undergraduate thesis project on the psychology of scientific fraud. I have already planned out (and preregistered) my data analysis plan (see [here](https://benjaminjzubaly.github.io/The-Psychology-of-Scientific-Fraud-Thesis/#data-analysis "Thesis Data Analysis Plan")), and I will note throughout the document if I deviate from this plan and why. This project is also being tracked in a private GitHub repository in case I need to revert back to a previous version of the project due to a fatal error.

## Overview of Sections

-   **Data Cleaning:** The data cleaning section will take the finalized dataset that I have included in the project directory and use it to create the remaining variables that we need for our analysis. We will also save this dataset.

-   **Data Exploration:** In the data exploration section, we will check for and deal with missing data, run descriptive statistics of our outcome variables, visualize our data, and run bivariate correlations.

-   **Testing Hypotheses:** In the hypothesis testing section, we will test each of our hypotheses one-by-one, according to our analysis plan.

## Directory Set-Up

In order to ensure that this analysis is computationally reproducible, I have included everything that is needed to complete this analysis (just the finalized data file, "study_dataset.csv") in the current working directory. The code will be displayed before the output of each analysis.

## Variable Definitions

Although the following variables may not exist until after the data cleaning section, here is what each variable name refers to, so that you can refer to them while examining the code.

`DOI`: Unique identifier for each paper (i.e., the paper DOI).

`PaperType`: Categorical variable indicating SAFP, SAGP, MAFP, or MAGP.

`LingObf`: Continuous variable for linguistic obfuscation.

`CertSent`: Continuous variable for certainty sentiment.

`Refs`: Count variable for references.

`FraudCorrAuth`: Dichotomous variable indicating if the fraudulent author is the corresponding author (1) or is not (0). Unknown cases will be marked in a seperate variable with this variable left blank.

`NumAuth`: Count variable indicating the number of authors for each paper.

`abstraction`: Abstraction index composed of the sum of standardized scores for `article`, `prep`, and `quantity`.

`article`: Articles from LIWC.

`prep`: Prepositions from LIWC.

`quantity`: Quantities from LIWC.

`cause`: Causation terms from LIWC.

`jargon`: The percent of words not captured by LIWC (100-`Dic`).

`Dic`: The percentage of words captured by all LIWC dictionaries.

`emo_pos`: Positive emotion terms from LIWC.

`flesch_re`: Flesch Reading Ease from ARTE.

## Initial Package Installations

If you would like to reproduce this analysis, here are the package I will be using, so that they can be cued before starting.

```{r}
install.packages("readr")       # For reading data
install.packages("dplyr")       # For data manipulation and handling of missing data
install.packages("psych")       # For descriptive statistics, correlations
install.packages("ggplot2")     # For data visualization
install.packages("car")         # For diagnostic tests such as Levene's test
install.packages("rcompanion")  # For Games-Howell post-hoc test (if applicable)
install.packages("dunn.test")   # For Dunn post-hoc test (if applicable)
```

## Initial Import of Data

We will not load in the dataset "study_dataset.csv" from the working directory.

```{r}
library(readr) # Loading the readr package

data <- read_csv("study_dataset.csv") # Loading in study dataset as "data"
```

I have viewed the data frame, and the data seems to have loaded correctly.

# Data Cleaning

To conduct the analysis, we will first need to calculate the `LingObf` variable by calculating the `abstraction` index and `jargon` words; creating standardized scores for `abstraction`, `cause`, `jargon`, `emo_pos`, and `flesch_re`; and calculating the `LingObf` composite variable from these standardized scores.

1.  First, we will calculate the `abstraction` index by creating standardized scores for `article`, `prep`, and `quantity` and summing them.

```{r}
# Calculate standardized scores for article, prep, and quantity and add them to the dataset
data$articles_standardized <- scale(data$article)
data$prep_standardized <- scale(data$prep)
data$quantity_standardized <- scale(data$quantity)

# Create the new variable 'abstraction' as the sum of the three standardized variables
data$abstraction <- (data$articles_standardized + data$prep_standardized + data$quantity_standardized)
```

-   After viewing the data, the transformations and variable calculation seem to have occurred appropriately.

2.  Next, we will calculate the `jargon` words by subtracting `Dic` from 100.

```{r}
# Calculate the new variable 'jargon' by subtracting 'Dic' from 100
data$jargon <- (100 - data$Dic)
```

-   After viewing the data, the variable calculation seem to have occurred appropriately.

3.  Next, we will create standardized scores for each subcomponent of the `LingObf`.

```{r}
# Standardize the new set of variables and add them to the dataset
data$abstraction_standardized <- scale(data$abstraction)
data$cause_standardized <- scale(data$cause)
data$jargon_standardized <- scale(data$jargon)
data$emo_pos_standardized <- scale(data$emo_pos)
data$flesch_re_standardized <- scale(data$flesch_re)
```

-   After viewing the data, the variable transformations seem to have occurred appropriately.

4.  Now we will calculate the `LingObf` variable using the following formula: \[cause_standardized + abstraction_standardized + jargon_standardized\] – \[emo_pos_standardized + flesch_re_standardized\].

```{r}
# Calculate 'LingObf'
data$LingObf <- (data$cause_standardized + data$abstraction_standardized + data$jargon_standardized) - (data$emo_pos_standardized + data$flesch_re_standardized)
```

-   After viewing the data, the variable calculation seem to have occurred appropriately.

5.  Lastly, our variable that indicates certainty sentiment is currently `certainty_avg`, but to make things easier I am going to copy this data into a new variables called `CertSent`.

```{r}
data$CertSent <- data$certainty_avg
```

To ensure that our clean data is saved, we will write the dataset to the current working directory.

```{r}
# Writing our data as a csv file in the current working directory
write.csv(data, "clean_study_data.csv")
```

-   I have opened the saved data file outside of Rstudio, and it seems to have been written correctly.

# Data Exploration

## Dealing with Missing Data

1.  Data will be first inspected for missing scores.

```{r}
library(dplyr) # Loading the dplyr package for data manipulation and handling missing values

# To summarize the number of missing values in each column
missing_data_summary <- sapply(data, function(x) sum(is.na(x)))

print(missing_data_summary) # To see summary of missing values for all columns
```

-   Taking a look at our outcome variables, there are no missing scores, so we do not need to impute any values.

## Descriptive Statistics

1.  I will now generate descriptive statistics for each relevant variable in the dataset. I originally (in the preregistered plan) was simply going to deploy the `describe()` function on the entire dataset, but because I retained all of the columns from the Retraction Watch Database [@retracti2023] and all of the output from the text analysis packages [@aggarwal2022; @boyd2022; @rocklage2023] there are currently 219 variables. Because this would be unmanageable, I am going to only calculate descriptive statistics for a selection of variables of interest. I will first create a dataframe with only the continuous variables that I am interested in generating descriptive statistics for, and I will use the `psych` package to produce the descriptive statistics for these variables.

```{r}
library(psych) # Loading the psych package

# Selecting the continuous variables I am interested in getting descriptive statistics for (plus paper type for the next part)
continuous_data_for_descriptives <- data[c("year", "Refs", "flesch_re", "WC", "abstraction", "jargon", "CertSent", "LingObf", "cause", "emo_pos", "article", "prep", "quantity", "PaperType")]

# Generating descriptive statistics for variables of interest
continuous_descriptive_stats_all <- describe(continuous_data_for_descriptives)

# Displaying the results of the descriptive stats for the variables listed above
continuous_descriptive_stats_all
```

-   I won't make too many comments here, because for most of these measures there are not really formal or informal norms against which to judge them. That being said:
    -   **Year:** The mean year is mid-2009, with a standard deviation of 10 years (max 2022 and min 1980), indicating that the sample is recent enough to be relevant but also spans quite a number of years. This I think is good insofar as the recent history of academic publishing is represented more fully (some recent investigations limited their search to the three years prior to publication). There is some negative skew, which I suspect is due to the 1980 paper being quite a bit older than most papers.
    -   **Refs:** The mean number of references is 45.82 with a standard deviation of 23.47. At least intuitively, this seems like a pretty standard distribution of references if we were to randomly select papers from the literature. However, the range is huge, with one paper showing 146 references—perhaps why the skew is positive. This may be something to consider when making group comparisons, as this point may have significant leverage.
    -   
-   Now we will create descriptive statistics for each of the continuous variables above within the `PaperType` groups.

```{r}
# Making PaperType a factor variable in the continuous variable dataframe to allow for grouping
continuous_data_for_descriptives$PaperType <- factor(continuous_data_for_descriptives$PaperType, levels = c("SAFP", "MAFP", "SAGP", "MAGP"), labels = c("Single-Authored Fraudulent Papers", "Multi-Authored Fraudulent Papers", "Single-Authored Genuine Papers", "Multi-Authored Genuine Papers"))

# Generating descriptive statistics within PaperType groups
descriptive_stats_by_PaperType <- describeBy(continuous_data_for_descriptives, group = continuous_data_for_descriptives$PaperType)

descriptive_stats_by_PaperType
```

2.  Frequency tables will now be produced for categorical variables, both for the data in general and within `PaperType` groups. A proportion table will be produced to more easily compare frequencies across `PaperType` groups.

-   First, we will make the variables `inst_pres`, `gender`, `simple_reason`, `Country`, and `PaperType` factor variables.

```{r}
# Changing inst_pres (institutional prestige), gender, simple_reason, Country, and PaperType into factor variables
data$inst_pres <- factor(data$inst_pres, levels = c(0, 1), labels = c("Not Major Research Institution", "Major Research Institution"))
data$gender <- factor(data$gender, levels = c("FEMALE", "MALE"), labels = c("Female", "Male"))
data$simple_reason <- factor(data$simple_reason, levels = c("f_data", "f_image", "m_image", "f_data f_image"), labels = c("Fabricated/Falsified Data", "Fabricated/Falsified Image", "Manipulated Image", "Fabricated/Falsified Data and Image"))
data$Country <- factor(data$Country)
data$PaperType <- factor(data$PaperType)
```

-   Next, we will produce frequency tables for each categorical variable of interest for the whole dataset.

```{r}
# Creating the frequency tables for each categorical variable
freq_tab_inst_pres <- table(data$inst_pres)
freq_tab_gender <- table(data$gender)
freq_tab_simple_reason <- table(data$simple_reason)
freq_tab_Country <- table(data$Country)
freq_tab_PaperType <- table(data$PaperType)

# Displaying the frequency tables
freq_tab_inst_pres
freq_tab_gender
freq_tab_simple_reason
freq_tab_Country
freq_tab_PaperType
```
